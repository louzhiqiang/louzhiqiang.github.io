<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>痒痒 团团 和 咘咘</title>
  
  <subtitle>一切都是最好的安排</subtitle>
  <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/atom.xml" rel="self"/>
  
  <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/"/>
  <updated>2022-10-30T12:53:22.108Z</updated>
  <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/</id>
  
  <author>
    <name>zhiqiang.lou</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>clickHouse初识</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/30/clickHouse%E5%88%9D%E8%AF%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/30/clickHouse%E5%88%9D%E8%AF%86/</id>
    <published>2022-10-30T12:50:22.000Z</published>
    <updated>2022-10-30T12:53:22.108Z</updated>
    
    <content type="html"><![CDATA[<h2 id="clickhouse初识"><a href="#clickhouse初识" class="headerlink" title="clickhouse初识"></a>clickhouse初识</h2><p>参考：<br><a href="https://developer.aliyun.com/article/762097?spm=5176.20128342.J_6302206100.1.7c227ba2IPjLHw" target="_blank" rel="noopener">ClickHouse深度揭秘</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;clickhouse初识&quot;&gt;&lt;a href=&quot;#clickhouse初识&quot; class=&quot;headerlink&quot; title=&quot;clickhouse初识&quot;&gt;&lt;/a&gt;clickhouse初识&lt;/h2&gt;&lt;p&gt;参考：&lt;br&gt;&lt;a href=&quot;https://develo</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="clickhouse" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>sql面试题汇总</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/27/sql%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/27/sql%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/</id>
    <published>2022-10-27T11:17:31.000Z</published>
    <updated>2022-10-27T13:18:30.350Z</updated>
    
    <content type="html"><![CDATA[<p>sql面试题是一个关注不是特别多的地方，很多sql的写法其实也是需要融入算法里的点，尤其是一些分析场景，需要去补全数据。<br>这里把这个链接沉淀下来，后边有需要的时候可以看一下。</p><p><a href="https://blog.51cto.com/u_14932245/4837359" target="_blank" rel="noopener">https://blog.51cto.com/u_14932245/4837359</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;sql面试题是一个关注不是特别多的地方，很多sql的写法其实也是需要融入算法里的点，尤其是一些分析场景，需要去补全数据。&lt;br&gt;这里把这个链接沉淀下来，后边有需要的时候可以看一下。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.51cto.com/u_14932</summary>
      
    
    
    
    <category term="面试" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
  </entry>
  
  <entry>
    <title>doris查询原理</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/16/doris%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/10/16/doris%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86/</id>
    <published>2022-10-16T08:17:01.000Z</published>
    <updated>2022-10-16T13:22:46.951Z</updated>
    
    <content type="html"><![CDATA[<h1 id="doris查询原理"><a href="#doris查询原理" class="headerlink" title="doris查询原理"></a>doris查询原理</h1><p>doris的上层mpp部分的查询主要分为四个过程：Analyze，SinglePlan，DistributedPlan，Schedule。这个过程跟presto的过程差不多，两者可以对比这学习。</p><p>Analyze 负责对 AST 进行前期的一些处理，SinglePlan 根据 AST 进行优化生成单机查询计划，DistributedPlan 将单机的查询计划拆成分布式的查询计划，Schedule 阶段负责决定查询计划下发到哪些机器上执行。</p><h2 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h2><p>1、最大化计算的并行性：如何划分stage</p><p>2、最小化数据的网络传输：如何分配任务以及中间数据的存储</p><p>3、最大化减少需要扫描的数据：如何做到更加彻底的下推</p><h2 id="查询总架构"><a href="#查询总架构" class="headerlink" title="查询总架构"></a>查询总架构</h2><p><img src="/images/4326a9cad256609a4de27912ddc07e56.png" alt=""></p><p>Analyze 负责对 AST 进行前期的一些处理，SinglePlan 根据 AST 进行优化生成单机查询计划，DistributedPlan 将单机的查询计划拆成分布式的查询计划，Schedule 阶段负责决定查询计划下发到哪些机器上执行。</p><p><img src="/images/d1668d932ee4abf6ecf178b14661cacf.png" alt=""></p><h2 id="sql解析"><a href="#sql解析" class="headerlink" title="sql解析"></a>sql解析</h2><p>词法分析采用 jflex 技术，语法分析采用 java cup parser 技术，最后生成抽象语法树（Abstract Syntax Tree）AST，这些都是现有的、成熟的技术。</p><p>逻辑计划阶段，是从AST转化为代数关系的阶段，代数关系是一棵算子树，每个节点代表一种对数据的计算方式，整棵树代表了数据的计算方式以及流动方向。</p><p>如下：</p><p><img src="/images/a8cfc820f7d4e211c821253e5c9bcf84.png" alt=""></p><blockquote><p>这个方式结合flink的数据流转方式进行思考。</p></blockquote><p>物理计划是在逻辑计划的基础上，根据机器的分布，数据的分布，决定去哪些机器上执行哪些计算操作。</p><p>Doris 系统的 SQL 解析也是采用这些步骤，只不过根据 Doris 系统结构的特点和数据的存储方式，进行了细化和优化，最大化发挥机器的计算能力。</p><h3 id="sql-parser"><a href="#sql-parser" class="headerlink" title="sql parser"></a>sql parser</h3><p>AST 是一种树状结构，代表着一条 SQL。不同类型的查询 select, insert, show, set, alter table, create table 等经过 Parse 阶段后生成不同的数据结构（SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt 等），但他们都继承自 Statement，并根据自己的语法规则进行一些特定的处理。例如：对于 select 类型的 sql， Parse 之后生成了 SelectStmt 结构。</p><p>SelectStmt 结构包含了 SelectList，FromClause，WhereClause，GroupByClause，SortInfo 等结构。这些结构又包含了更基础的一些数据结构，如 WhereClause 包含了 BetweenPredicate（between 表达式）, BinaryPredicate（二元表达式）， CompoundPredicate（and or 组合表达式）, InPredicate（in 表达式）等。</p><p><img src="/images/340f57691764bf70c9601a15631e0005.jpeg" alt=""></p><blockquote><p>以上结构基于不同的解析工具会有不同的节点类型，但是大体结构是一致的。</p></blockquote><h3 id="analyze"><a href="#analyze" class="headerlink" title="analyze"></a>analyze</h3><p>抽象语法树是由 StatementBase 这个抽象类表示。这个抽象类包含一个最重要的成员函数 analyze()，用来执行 Analyze 阶段要做的事。</p><p>不同类型的查询 select, insert, show, set, alter table, create table 等经过 Parse 阶段后生成不同的数据结构（SelectStmt, InsertStmt, ShowStmt, SetStmt, AlterStmt, AlterTableStmt, CreateTableStmt 等），这些数据结构继承自 StatementBase，并实现 analyze() 函数，对特定类型的 SQL 进行特定的 Analyze。</p><p>例如：select 类型的查询，会转成对 select sql 的子语句 SelectList, FromClause, GroupByClause, HavingClause, WhereClause, SortInfo 等的 analyze()。然后这些子语句再各自对自己的子结构进行进一步的 analyze()，通过层层迭代，把各种类型的 sql 的各种情景都分析完毕。例如：WhereClause 进一步分析其包含的 BetweenPredicate（between 表达式）, BinaryPredicate（二元表达式）， CompoundPredicate（and or 组合表达式）, InPredicate（in 表达式）等。</p><blockquote><p>这种解析设计方式是值得借鉴思考的，sql上层会按照不同的业务类型进行不同的具体实现，然后再确定了上层业务之后，底层根据上层的语境进行下属层次的迭代分析。每一种类型都有自己的结构，然后每种结构也会有自己的解析，直到遇到无法解析的就可以进行反馈。</p></blockquote><p><strong>对于查询类型的 SQL，包含以下几项重要工作：</strong></p><ul><li><p><strong>元信息的识别和解析：</strong>识别和解析 sql 中涉及的 Cluster, Database, Table, Column 等元信息，确定需要对哪个集群的哪个数据库的哪些表的哪些列进行计算。</p></li><li><p><strong>SQL 的合法性检查：</strong>窗口函数不能 DISTINCT，投影列是否有歧义，where 语句中不能含有 grouping 操作等。</p></li><li><p><strong>SQL 简单重写：</strong>比如将 select * 扩展成 select 所有列，count distinct 转成 bitmap 或者 hll 函数等。</p></li><li><p><strong>函数处理：</strong>检查 sql 中包含的函数和系统定义的函数是否一致，包括参数类型，参数个数等。</p></li><li><p><strong>Table 和 Column 的别名处理</strong></p></li><li><p><strong>类型检查和转换：</strong>例如二元表达式两边的类型不一致时，需要对其中一个类型进行转换（BIGINT 和 DECIMAL 比较，BIGINT 类型需要 Cast 成 DECIMAL）。</p></li></ul><blockquote><p>以上每个环节都是有各自作用的，而且每一部都很重要，自己之前对于每一步骤其实都有涉及，但是把每个步骤更好的划分，做得不够，可以参考这个架构，好的架构对于规则的扩展很重要。</p></blockquote><p>对 AST 进行 analyze 后，会再进行一次 rewrite 操作，进行精简或者是转成统一的处理方式。<strong>目前 rewrite 的算法是基于规则的方式，针对 AST 的树状结构，自底向上，应用每一条规则进行重写。如果重写后，AST 有变化，则再次进行 analyze 和 rewrite，直到 AST 无变化为止。</strong></p><p>例如：常量表达式的化简：1 + 1 + 1 重写成 3，1 &gt; 2 重写成 Flase 等。将一些语句转成统一的处理方式，比如将 where in, where exists 重写成 semi join, where not in, where not exists 重写成 anti join。</p><h3 id="生成单机逻辑-Plan-阶段"><a href="#生成单机逻辑-Plan-阶段" class="headerlink" title="生成单机逻辑 Plan 阶段"></a><strong>生成单机逻辑 Plan 阶段</strong></h3><p>这部分工作主要是根据 AST 抽象语法树生成代数关系，也就是俗称的算子数。树上的每个节点都是一个算子，代表着一种操作。</p><p>比如常见的算子操作：scan、sort、exchange、hashJoin、project等</p><p><strong>具体来说这个阶段主要做了如下几项工作：</strong></p><ul><li><p><strong>Slot 物化：</strong>指确定一个表达式对应的列需要 Scan 和计算，比如聚合节点的聚合函数表达式和 Group By 表达式需要进行物化。</p></li><li><p><strong>投影下推：</strong>BE 在 Scan 时只会 Scan 必须读取的列。</p></li><li><p><strong>谓词下推：</strong>在满足语义正确的前提下将过滤条件尽可能下推到 Scan 节点。</p></li><li><p><strong>分区，分桶裁剪：</strong>根据过滤条件中的信息，确定需要扫描哪些分区，哪些桶的 tablet。</p></li><li><p><strong>Join Reorder：</strong>对于 Inner Join, Doris 会根据行数调整表的顺序，将大表放在前面。</p></li><li><p><strong>Sort + Limit 优化成 TopN：</strong>对于 order by limit 语句会转换成 TopN 的操作节点，方便统一处理。</p></li><li><p><strong>MaterializedView 选择：</strong>会根据查询需要的列，过滤，排序和 Join 的列，行数，列数等因素选择最佳的物化视图。</p></li></ul><blockquote><p>基于以上优化，可以最大程度减少分布式执行的资源消耗。其实在规则上还远远不止以上这些。</p></blockquote><h3 id="生成分布式-Plan-阶段"><a href="#生成分布式-Plan-阶段" class="headerlink" title="生成分布式 Plan 阶段"></a><strong>生成分布式 Plan 阶段</strong></h3><p>有了单机的 PlanNode 树之后，就需要进一步根据分布式环境，拆成分布式 PlanFragment 树（PlanFragment 用来表示独立的执行单元），毕竟一个表的数据分散地存储在多台主机上，完全可以让一些计算并行起来。</p><blockquote><p>这个计划决定了一个MPP架构数据服务可以支持的场景。</p></blockquote><p>这个步骤的主要目标是<strong>最大化并行度和数据本地化</strong>。主要方法是<strong>将能够并行执行的节点拆分出去单独建立一个 PlanFragment，用 ExchangeNode 代替被拆分出去的节点，用来接收数据</strong>。拆分出去的节点增加一个 DataSinkNode，用来将计算之后的数据传送到 ExchangeNode 中，做进一步的处理。</p><p>这个过程跟spark的物理计划很像，flink的物理计划产出也是这样的，都是在stage之间进行拆解的时候，进行一些输入和输出的节点的增加。</p><blockquote><p>这个过程跟后端架构的搭建很像，增加中间节点往往就可以进行解耦，但是如何做减法，面对的场景可以有更好的扩展性，才是这里最难的。</p></blockquote><p>这一步采用递归的方法，自底向上，遍历整个 PlanNode 树，然后给树上的每个叶子节点创建一个 <strong>PlanFragment</strong>，如果碰到父节点，则考虑将其中能够并行执行的子节点拆分出去，父节点和保留下来的子节点组成一个<strong>parent PlanFragment</strong>。拆分出去的子节点增加一个父节点 DataSinkNode 组成一个 child PlanFragment，child PlanFragment 指向 parent PlanFragment。这样就确定了<strong>数据的流动方向</strong>。</p><p>以join为例：</p><p><strong>Doris 目前支持 4 种 join 算法：</strong>broadcast join，hash partition join，colocate join，bucket shuffle join。</p><p><strong>broadcast join：</strong>将小表发送到大表所在的每台机器，然后进行 hash join 操作。当一个表扫描出的数据量较少时，计算 broadcast join 的 cost，通过计算比较 hash partition 的 cost，来选择 cost 最小的方式。</p><p><strong>hash partition join：</strong>当两张表扫描出的数据都很大时，一般采用 hash partition join。它遍历表中的所有数据，计算 key 的哈希值，然后对集群数取模，选到哪台机器，就将数据发送到这台机器进行 hash join 操作。</p><p><strong>colocate join：</strong>两个表在创建的时候就指定了数据分布保持一致，那么当两个表的 join key 与分桶的 key 一致时，就会采用 colocate join 算法。由于两个表的数据分布是一样的，那么 hash join 操作就相当于在本地，不涉及到数据的传输，极大提高查询性能。</p><p><strong>bucket shuffle join：</strong>当 join key 是分桶 key，并且只涉及到一个分区时，就会优先采用 bucket shuffle join 算法。由于分桶本身就代表了数据的一种切分方式，所以可以利用这一特点，只需将右表对左表的分桶数 hash 取模，这样只需网络传输一份右表数据，极大减少了数据的网络传输，如下图所示bucket shuffle join 示例。</p><p><img src="/images/91e573c81f25c23cb42d967dfb7692be.png" alt=""></p><p>下边看一下HashJoinNode的实现逻辑：</p><ul><li>对 PlanNode，自底向上创建 PlanFragment。</li><li>如果是 ScanNode，则直接创建一个 PlanFragment，PlanFragment 的 RootPlanNode 是这个 ScanNode。</li><li>如果是 HashJoinNode，则首先计算下 broadcastCost，为选择 boracast join 还是 hash partition join 提供参考。</li><li>根据不同的条件判断选择哪种 Join 算法</li><li>如果使用 colocate join，由于 join 操作都在本地，就不需要拆分。设置 HashJoinNode 的左子节点为 leftFragment 的 RootPlanNode，右子节点为 rightFragment 的 RootPlanNode，与 leftFragment 共用一个 PlanFragment，删除掉 rightFragment。</li><li>如果使用 bucket shuffle join，需要将右表的数据发送给左表。所以先创建了一个 ExchangeNode，设置 HashJoinNode 的左子节点为 leftFragment 的 RootPlanNode，右子节点为这个 ExchangeNode，与 leftFragment 共用一个 PlanFragment，并且指定 rightFragment 数据发送的目的地为这个 ExchangeNode。</li><li>如果使用 broadcast join，需要将右表的数据发送给左表。所以先创建了一个 ExchangeNode，设置 HashJoinNode 的左子节点为 leftFragment 的 RootPlanNode，右子节点为这个 ExchangeNode，与 leftFragment 共用一个 PlanFragment，并且指定 rightFragment 数据发送的目的地为这个 ExchangeNode。</li><li>如果使用 hash partition join，左表和右边的数据都要切分，需要将左右节点都拆分出去，分别创建 left ExchangeNode, right ExchangeNode，HashJoinNode 指定左右节点为 left ExchangeNode 和 right ExchangeNode。单独创建一个 PlanFragment，指定 RootPlanNode 为这个 HashJoinNode。最后指定 leftFragment, rightFragment 的数据发送目的地为 left ExchangeNode, right ExchangeNode。</li></ul><p>以下就是上边的过程：</p><p><img src="/images/4197d9e0e18a4d38334937d0bf45e372.png" alt=""></p><p>下图是两个表的 join 操作转换成 PlanFragment 树之后的示例，一共生成了 3 个 PlanFragment。最终数据的输出通过 ResultSinkNode 节点。</p><p><img src="/images/2cf7964d4cb3001834e57532d467ad8d.png" alt=""></p><h3 id="Schedule-阶段"><a href="#Schedule-阶段" class="headerlink" title="Schedule 阶段"></a><strong>Schedule 阶段</strong></h3><p>这一步是根据分布式逻辑计划，创建分布式物理计划。主要解决以下问题：</p><ul><li>哪个 BE 执行哪个 PlanFragment</li><li>每个 Tablet 选择哪个副本去查询</li><li>如何进行多实例并发</li></ul><p>创建物理计划的核心流程：</p><p><img src="/images/f217f1b8f4321424f66d4679b5367945.png" alt=""></p><p><strong>prepare 阶段：</strong>给每个 PlanFragment 创建一个 FragmentExecParams 结构，用来表示 PlanFragment 执行时所需的所有参数；如果一个 PlanFragment 包含有 DataSinkNode，则找到数据发送的目的 PlanFragment，然后指定目的 PlanFragment 的 FragmentExecParams 的输入为该 PlanFragment 的 FragmentExecParams。</p><p> <strong>computeScanRangeAssignment 阶段：</strong>针对不同类型的 join 进行不同的处理。</p><ul><li><strong>computeScanRangeAssignmentByColocate：</strong>针对 colocate join 进行处理，由于 join 的两个表桶中的数据分布都是一样的，他们是基于桶的 join 操作，所以在这里是确定每个桶选择哪个 host。在给 host 分配桶时，尽量保证每个 host 分配到的桶基本平均。</li><li><strong>computeScanRangeAssignmentByBucket：</strong>针对 bucket shuffle join 进行处理，也只是基于桶的操作，所以在这里是确定每个桶选择哪个 host。在给 host 分配桶时，同样需要尽量保证每个 host 分配到的桶基本平均。</li><li><strong>computeScanRangeAssignmentByScheduler：</strong>针对其他类型的 join 进行处理。确定每个 scanNode 读取 tablet 哪个副本。一个 scanNode 会读取多个 tablet，每个 tablet 有多个副本。为了使 scan 操作尽可能分散到多台机器上执行，提高并发性能，减少 IO 压力，Doris 采用了 Round-Robin 算法，使 tablet 的扫描尽可能地分散到多台机器上去。例如 100 个 tablet 需要扫描，每个 tablet 3 个副本，一共 10 台机器，在分配时，保障每台机器扫描 10 个 tablet。</li></ul><p><strong>computeFragmentExecParams 阶段：</strong>这个阶段解决 PlanFragment 下发到哪个 BE 上执行，以及如何处理实例并发问题。确定了每个 tablet 的扫描地址之后，就可以以地址为维度，将 FragmentExecParams 生成多个实例，也就是 FragmentExecParams 中包含的地址有多个，就生成多个实例 FInstanceExecParam。如果设置了并发度，那么一个地址的执行实例再进一步的拆成多个 FInstanceExecParam。针对 bucket shuffle join 和 colocate join 会有一些特殊处理，但是基本思想一样。FInstanceExecParam 创建完成后，会分配一个唯一的 ID，方便追踪信息。如果 FragmentExecParams 中包含有 ExchangeNode，需要计算有多少 senders，以便知道需要接受多少个发送方的数据。最后 FragmentExecParams 确定 destinations，并把目的地址填充上去。</p><p><strong>create result receiver 阶段：</strong>result receiver 是查询完成后，最终数据需要输出的地方。</p><p> <strong>to thrift 阶段：</strong>根据所有 PlanFragment 的 FInstanceExecParam 创建 rpc 请求，然后下发到 BE 端执行。这样一个完整的 SQL 解析过程完成了。</p><p>下图是一个简单的事例：</p><p><img src="/images/b2a6b4950911a95e715154778b1777a4.png" alt=""></p><p>图中的 PlanFrament 包含了一个 ScanNode，ScanNode 扫描 3 个 tablet，每个 tablet 有 2 副本，集群假设有 2 台 host。</p><p>computeScanRangeAssignment 阶段确定了需要扫描 replica 1,3,5,8,10,12，其中 replica 1,3,5 位于 host1 上，replica 8,10,12 位于 host2 上。</p><p>如果全局并发度设置为 1 时，则创建 2 个实例 FInstanceExecParam，下发到 host1 和 host2 上去执行，如果如果全局并发度设置为 3，这个 host1 上创建 3 个实例 FInstanceExecParam，host2 上创建 3 个实例 FInstanceExecParam，每个实例扫描一个 replica，相当于发起 6 个 rpc 请求。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>每一步骤都很重要，然而针对并发度的设置以及不同的文件布局，实际进行调度的时候，会有很多细节，所以mpp架构的数据查询，最有差异的就是这个调度算法以及可以提供的上下文的差异，不同的上下文可以支持的算法是不一样的，这个部分可以多多思考。</p><blockquote><p>注意上边doris的原则</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;doris查询原理&quot;&gt;&lt;a href=&quot;#doris查询原理&quot; class=&quot;headerlink&quot; title=&quot;doris查询原理&quot;&gt;&lt;/a&gt;doris查询原理&lt;/h1&gt;&lt;p&gt;doris的上层mpp部分的查询主要分为四个过程：Analyze，SinglePla</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="doris" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/doris/"/>
    
  </entry>
  
  <entry>
    <title>spark优化案例</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/26/spark%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/26/spark%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B/</id>
    <published>2022-09-26T11:01:39.000Z</published>
    <updated>2022-09-26T12:30:40.430Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark优化案例"><a href="#spark优化案例" class="headerlink" title="spark优化案例"></a>spark优化案例</h1><h2 id="网易基于-Kyuubi-Spark-内核优化以及实践"><a href="#网易基于-Kyuubi-Spark-内核优化以及实践" class="headerlink" title="网易基于 Kyuubi + Spark 内核优化以及实践"></a>网易基于 Kyuubi + Spark 内核优化以及实践</h2><p>一、网易基于AQE的优化。网易有参与到AQE的框架开发中。<br>AQE在spark 2.X就有了初步想法，但是设计简陋，存在很多bug。后来在3.X版本，intel提出了AQE框架：</p><ul><li>对于shuffle reader的优化<ul><li>join倾斜优化</li><li>local shuffle reader    </li></ul></li><li>通过优化执行计划来进一步优化sql的执行性能</li></ul><p>spark 3.2 版本对于AQE是默认开启的。</p><p>AQE 可以解决数据倾斜、小文件、空分区。倾斜的解决方式就是拆开，而小文件的方式就是合并，空分区在小文件这个策略里是可以直接解决掉的。</p><p>AQE可以进行join策略调整，在初始map阶段之后的统计数据，就可以根据这些信息对执行计划做调整。</p><h2 id="kyuubi-spark"><a href="#kyuubi-spark" class="headerlink" title="kyuubi + spark"></a>kyuubi + spark</h2><p>kyuubi对外的作用：</p><ul><li>对外多种接口方式</li><li>多租户</li><li>云原生</li></ul><p>kyuubi分为server 和 engine两个部分，后者已经支持了spark、trino、flink。前者就是集成了多租户以及云原生。</p><p>kyuubi支持了基于proxy类型的long live的行为，这样就可以维护票据问题，也就避免了票据失效的问题。</p><h2 id="对数据质量的优化"><a href="#对数据质量的优化" class="headerlink" title="对数据质量的优化"></a>对数据质量的优化</h2><p>数据质量的两个角度：</p><ul><li>数据压缩率</li><li>数据文件，期望尽量跟hdfs的block契合。</li></ul><p>数据质量好的产出，对于后续的读以及当前数据集合本身的特点，都会有一个比较好的稳定性。</p><p>基于上边kyuubi的方案，有两个优化方向：</p><ul><li>distribute by + local sort<br>存在隐患，比如数据倾斜，因为distribute本身还是会把热键放到一个分区里。<br>local sort对于少数字段的数据影响比较有效果，但是对于多维数据进行local sort，效果会打折扣。</li><li>Rebalance + Z-Order<br>rebalance 在 AQE 框架下提供小文件合并和大文件拆分的功能，而且可以针对写stage做单独的配置，这样针对hdfs的block使用可以尽量利用。<br>Z-Order 本质就是把多维数据可以映射到一维数据，在映射的过程中，可以保证这整个多维数据聚集分布效果不失真。在保证压缩率的前提下，对于后续的data skipping有保障。</li></ul><p>这一套方案对于其他引擎的查询优化是通用的，比如impala、trino等。</p><p><a href="https://appukvkryx45804.pc.xiaoe-tech.com/detail/i_633016dfe4b050af23bc1ede/1?from=p_618dcca0e4b0c005c9902656&fromH5=true&type=6" target="_blank" rel="noopener">方案链接</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spark优化案例&quot;&gt;&lt;a href=&quot;#spark优化案例&quot; class=&quot;headerlink&quot; title=&quot;spark优化案例&quot;&gt;&lt;/a&gt;spark优化案例&lt;/h1&gt;&lt;h2 id=&quot;网易基于-Kyuubi-Spark-内核优化以及实践&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>jvm内存结构</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/jvm%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/jvm%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/</id>
    <published>2022-09-24T13:09:02.000Z</published>
    <updated>2022-09-24T13:55:45.434Z</updated>
    
    <content type="html"><![CDATA[<h1 id="jvm内存结构"><a href="#jvm内存结构" class="headerlink" title="jvm内存结构"></a>jvm内存结构</h1><p>先看一张图，这张图能很清晰的说明JVM内存结构布局。<br><img src="https://images2015.cnblogs.com/blog/331425/201606/331425-20160623115840235-1252768148.png" alt=""></p><p>JVM内存结构主要有三大块：堆内存、方法区和栈。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；</p><p>方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；<br>栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。</p><p><img src="https://images2015.cnblogs.com/blog/331425/201606/331425-20160623115841781-223449019.png" alt=""></p><p>控制参数<br>-Xms设置堆的最小空间大小。<br>-Xmx设置堆的最大空间大小。<br>-XX:NewSize设置新生代最小空间大小。<br>-XX:MaxNewSize设置新生代最大空间大小。<br>-XX:PermSize设置永久代最小空间大小。<br>-XX:MaxPermSize设置永久代最大空间大小。<br>-Xss设置每个线程的堆栈大小。</p><p>老年代空间大小=堆空间大小-年轻代大空间大小</p><p><img src="https://images2015.cnblogs.com/blog/331425/201606/331425-20160623115846235-947282498.png" alt=""></p><p>方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。</p><h2 id="Java堆（Heap）"><a href="#Java堆（Heap）" class="headerlink" title="Java堆（Heap）"></a>Java堆（Heap）</h2><p>堆是jvm占用空间最大的一个，对象都是存储在这个空间。<br>如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。</p><h2 id="方法区（Method-Area）"><a href="#方法区（Method-Area）" class="headerlink" title="方法区（Method Area）"></a>方法区（Method Area）</h2><p>方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。<br>这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。<br>根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 </p><h2 id="程序计数器（Program-Counter-Register）"><a href="#程序计数器（Program-Counter-Register）" class="headerlink" title="程序计数器（Program Counter Register）"></a>程序计数器（Program Counter Register）</h2><p>程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。<br>由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，<strong>每条线程都需要有一个独立的程序计数器</strong>，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。</p><p>如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。</p><p>此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。</p><h2 id="JVM栈（JVM-Stacks）"><a href="#JVM栈（JVM-Stacks）" class="headerlink" title="JVM栈（JVM Stacks）"></a>JVM栈（JVM Stacks）</h2><p>与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。<strong>虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。</strong>每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 </p><p>局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。</p><p>其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，<strong>当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小</strong>。</p><p>在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出<strong>StackOverflowError异常</strong>；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出<strong>OutOfMemoryError异常</strong>。</p><h2 id="本地方法栈（Native-Method-Stacks）"><a href="#本地方法栈（Native-Method-Stacks）" class="headerlink" title="本地方法栈（Native Method Stacks）"></a>本地方法栈（Native Method Stacks）</h2><p>本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的<strong>Native方法</strong>服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;jvm内存结构&quot;&gt;&lt;a href=&quot;#jvm内存结构&quot; class=&quot;headerlink&quot; title=&quot;jvm内存结构&quot;&gt;&lt;/a&gt;jvm内存结构&lt;/h1&gt;&lt;p&gt;先看一张图，这张图能很清晰的说明JVM内存结构布局。&lt;br&gt;&lt;img src=&quot;https://im</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>Java中用户线程和守护线程</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/Java%E4%B8%AD%E7%94%A8%E6%88%B7%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/Java%E4%B8%AD%E7%94%A8%E6%88%B7%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B/</id>
    <published>2022-09-24T07:38:19.000Z</published>
    <updated>2022-09-24T07:49:32.174Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java中用户线程和守护线程"><a href="#Java中用户线程和守护线程" class="headerlink" title="Java中用户线程和守护线程"></a>Java中用户线程和守护线程</h1><p>Java 语言中无论是线程还是线程池，默认都是用户线程，因此用户线程也被成为普通线程。</p><p>以线程为例，想要查看线程是否为守护线程只需通过调用 isDaemon() 方法查询即可，如果查询的值为 false 则表示不为守护线程，自然也就属于用户线程了.</p><p>守护线程（Daemon Thread）也被称之为后台线程或服务线程，守护线程是为用户线程服务的，当程序中的用户线程全部执行结束之后，守护线程也会跟随结束。</p><h2 id="两者的区别"><a href="#两者的区别" class="headerlink" title="两者的区别"></a>两者的区别</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DaemonExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Thread thread = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">                    <span class="comment">// 打印 i 信息</span></span><br><span class="line">                    System.out.println(<span class="string">"i:"</span> + i);</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        <span class="comment">// 休眠 100 毫秒</span></span><br><span class="line">                        Thread.sleep(<span class="number">100</span>);</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 设置为守护线程</span></span><br><span class="line">        thread.setDaemon(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 启动线程</span></span><br><span class="line">        thread.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/950cff9bc03748599bcc418f8252e15c~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp" alt=""></p><p>可见守护线程在用户线程执行完毕也就直接销毁了，所以后续的打印就没有执行。</p><p>守护线程的使用需要注意以下三个问题：</p><ol><li>守护线程的设置 setDaemon(true) 必须要放在线程的 start() 之前，否则程序会报错。</li><li>在守护线程中创建的所有子线程都是守护线程。</li><li>使用 jojn() 方法会等待一个线程执行完，无论此线程是用户线程还是守护线程。</li></ol><h2 id="守护线程应用场景"><a href="#守护线程应用场景" class="headerlink" title="守护线程应用场景"></a>守护线程应用场景</h2><p><strong>守护线程的典型应用场景就是垃圾回收线程</strong>，当然还有一些场景也非常适合使用守护线程，比如服务器端的健康检测功能，对于一个服务器来说健康检测功能属于非核心非主流的服务业务，像这种为了主要业务服务的业务功能就非常合适使用守护线程，当程序中的主要业务都执行完成之后，服务业务也会跟随者一起销毁。</p><p>守护线程从业务逻辑层面来看权重比较低，但对于线程调度器来说无论是守护线程还是用户线程，在优先级相同的情况下被执行的概率都是相同的。守护线程的经典使用场景是垃圾回收线程，守护线程中创建的线程默认情况下也都是守护线程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Java中用户线程和守护线程&quot;&gt;&lt;a href=&quot;#Java中用户线程和守护线程&quot; class=&quot;headerlink&quot; title=&quot;Java中用户线程和守护线程&quot;&gt;&lt;/a&gt;Java中用户线程和守护线程&lt;/h1&gt;&lt;p&gt;Java 语言中无论是线程还是线程池，默认都</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>java锁粗化、锁消除、锁膨胀、自适应自旋锁</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/java%E9%94%81%E7%B2%97%E5%8C%96%E3%80%81%E9%94%81%E6%B6%88%E9%99%A4%E3%80%81%E9%94%81%E8%86%A8%E8%83%80%E3%80%81%E8%87%AA%E9%80%82%E5%BA%94%E8%87%AA%E6%97%8B%E9%94%81/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/java%E9%94%81%E7%B2%97%E5%8C%96%E3%80%81%E9%94%81%E6%B6%88%E9%99%A4%E3%80%81%E9%94%81%E8%86%A8%E8%83%80%E3%80%81%E8%87%AA%E9%80%82%E5%BA%94%E8%87%AA%E6%97%8B%E9%94%81/</id>
    <published>2022-09-24T03:18:47.000Z</published>
    <updated>2022-09-24T07:38:29.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="java锁粗化、锁消除、锁膨胀、自适应自旋锁"><a href="#java锁粗化、锁消除、锁膨胀、自适应自旋锁" class="headerlink" title="java锁粗化、锁消除、锁膨胀、自适应自旋锁"></a>java锁粗化、锁消除、锁膨胀、自适应自旋锁</h1><p>java在1.6版本中对于关键字 synchronized 进行了深度优化，主要优化以下四个特性：</p><ol><li>锁膨胀</li><li>锁消除</li><li>锁粗化</li><li>自适应自旋锁</li></ol><h2 id="锁膨胀"><a href="#锁膨胀" class="headerlink" title="锁膨胀"></a>锁膨胀</h2><p>所谓的锁膨胀是指 synchronized 从无锁升级到偏向锁，再到轻量级锁，最后到重量级锁的过程，它叫做锁膨胀也叫做锁升级。<br>JDK 1.6 之前，synchronized 是重量级锁，也就是说 synchronized 在释放和获取锁时都会从用户态转换成内核态，而转换的效率是比较低的。但有了锁膨胀机制之后，synchronized 的状态就多了无锁、偏向锁以及轻量级锁了，这时候在进行并发操作时，大部分的场景都不需要用户态到内核态的转换了，这样就大幅的提升了 synchronized 的性能。</p><h2 id="锁消除"><a href="#锁消除" class="headerlink" title="锁消除"></a>锁消除</h2><p>锁消除指的是在某些情况下，JVM 虚拟机如果检测不到某段代码被共享和竞争的可能性，就会将这段代码所属的同步锁消除掉，从而提高程序性能的目的。<br>锁消除的依据是逃逸分析的数据支持，如 StringBuffer 的 append() 方法，或 Vector 的 add() 方法，在很多情况下是可以进行锁消除的，比如以下这段代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">method</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    StringBuffer sb = <span class="keyword">new</span> StringBuffer();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        sb.append(<span class="string">"i:"</span> + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.nlark.com/yuque/0/2021/png/92791/1628004235900-ceda983f-f425-4d47-8dae-6cc7e193b882.png#align=left&display=inline&height=523&id=ua61627da&margin=%5Bobject%20Object%5D&name=image.png&originHeight=1046&originWidth=2120&size=217934&status=done&style=none&width=1060" alt=""></p><blockquote><p>stringBuilder是非线程安全的。stringBuffer是线程安全的。</p></blockquote><p>从上述结果可以看出，之前我们写的线程安全的加锁的 StringBuffer 对象，在生成字节码之后就被替换成了不加锁不安全的 StringBuilder 对象了，原因是 <strong>StringBuffer 的变量属于一个局部变量，并且不会从该方法中逃逸出去，所以此时我们就可以使用锁消除（不加锁）来加速程序的运行</strong>。</p><h2 id="锁粗化"><a href="#锁粗化" class="headerlink" title="锁粗化"></a>锁粗化</h2><p>锁粗化是指，将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。<br>我只听说锁“细化”可以提高程序的执行效率，也就是将锁的范围尽可能缩小，这样在锁竞争时，等待获取锁的线程才能更早的获取锁，从而提高程序的运行效率，但锁粗化是如何提高性能的呢？</p><p>没错，锁细化的观点在大多数情况下都是成立了，但是一系列连续加锁和解锁的操作，也会导致不必要的性能开销，从而影响程序的执行效率，比如这段代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">method</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        <span class="comment">// 伪代码：加锁操作</span></span><br><span class="line">        sb.append(<span class="string">"i:"</span> + i);</span><br><span class="line">        <span class="comment">// 伪代码：解锁操作</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们不考虑编译器优化的情况，如果在 for 循环中定义锁，那么锁的范围很小，但每次 for 循环都需要进行加锁和释放锁的操作，性能是很低的；但如果我们直接在 for 循环的外层加一把锁，那么对于同一个对象操作这段代码的性能就会提高很多，如下伪代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">method</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    <span class="comment">// 伪代码：加锁操作</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        sb.append(<span class="string">"i:"</span> + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 伪代码：解锁操作</span></span><br><span class="line">    <span class="keyword">return</span> sb.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>锁粗化的作用：<strong>如果检测到同一个对象执行了连续的加锁和解锁的操作，则会将这一系列操作合并成一个更大的锁，从而提升程序的执行效率</strong>。</p><h2 id="自适应自旋锁"><a href="#自适应自旋锁" class="headerlink" title="自适应自旋锁"></a>自适应自旋锁</h2><p>自旋锁是指通过自身循环，尝试获取锁的一种方式，伪代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 尝试获取锁</span></span><br><span class="line"><span class="keyword">while</span>(!isLock())&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>自旋锁优点在于它避免一些线程的挂起和恢复操作，因为挂起线程和恢复线程都需要从用户态转入内核态，这个过程是比较慢的，所以通过自旋的方式可以一定程度上避免线程挂起和恢复所造成的性能开销。</strong><br>但是，如果长时间自旋还获取不到锁，那么也会造成一定的资源浪费，所以我们通常会给自旋设置一个固定的值来避免一直自旋的性能开销。然而对于 synchronized 关键字来说，它的自旋锁更加的“智能”，synchronized 中的自旋锁是自适应自旋锁，这就好比之前一直开的手动挡的三轮车，而经过了 JDK 1.6 的优化之后，我们的这部“车”，一下子变成自动挡的兰博基尼了。</p><p>自适应自旋锁是指，<strong>线程自旋的次数不再是固定的值，而是一个动态改变的值，这个值会根据前一次自旋获取锁的状态来决定此次自旋的次数</strong>。比如上一次通过自旋成功获取到了锁，那么这次通过自旋也有可能会获取到锁，所以这次自旋的次数就会增多一些，而如果上一次通过自旋没有成功获取到锁，那么这次自旋可能也获取不到锁，所以为了避免资源的浪费，就会少循环或者不循环，以提高程序的执行效率。简单来说，<strong>如果线程自旋成功了，则下次自旋的次数会增多，如果失败，下次自旋的次数会减少</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;java锁粗化、锁消除、锁膨胀、自适应自旋锁&quot;&gt;&lt;a href=&quot;#java锁粗化、锁消除、锁膨胀、自适应自旋锁&quot; class=&quot;headerlink&quot; title=&quot;java锁粗化、锁消除、锁膨胀、自适应自旋锁&quot;&gt;&lt;/a&gt;java锁粗化、锁消除、锁膨胀、自适应自</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>spark共享变量</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/spark%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/24/spark%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/</id>
    <published>2022-09-24T03:06:26.000Z</published>
    <updated>2022-09-26T11:05:29.980Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark共享变量"><a href="#spark共享变量" class="headerlink" title="spark共享变量"></a>spark共享变量</h1><p>spark有两大共享变量：广播变量和累加器</p><p>累加器用来对信息进行聚合，而广播变量用来高效分发较大的对象。</p><h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>共享变量出现的原因：<br>通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。Spark 的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。</p><p>Spark 会自动把闭包中所有引用到的变量发送到工作节点上。虽然这很方便，但也很低效。原因有二：首先，默认的任务发射机制是专门为小任务进行优化的；其次，事实上你可能会在多个并行操作中使用同一个变量，但是 Spark 会为每个操作分别发送。</p><p><img src="https://img-blog.csdn.net/20180401185652753?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJvaWRfeHVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p>list是在driver端创建的，但是因为需要在excutor端使用，所以driver会把list以task的形式发送到excutor端，如果有很多个task，就会有很多给excutor端携带很多个list，如果这个list非常大的时候，就可能会造成内存溢出（如下图所示）。这个时候就引出了广播变量。</p><p><img src="https://img-blog.csdn.net/20180401185706513?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJvaWRfeHVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p>使用广播变量后：</p><p><img src="https://img-blog.csdn.net/20180401185715750?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJvaWRfeHVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p>使用广播变量的过程很简单：<br>(1) 通过对一个类型 T 的对象调用 SparkContext.broadcast 创建出一个 Broadcast[T] 对象。任何可序列化的类型都可以这么实现。<br>(2) 通过 value 属性访问该对象的值（在 Java 中为 value() 方法）。<br>(3) 变量只会被发到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BroadcastTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"broadcast"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hello java"</span>)</span><br><span class="line">    <span class="keyword">val</span> broadcast = sc.broadcast(list)</span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(<span class="string">"./word"</span>)</span><br><span class="line">    linesRDD.filter(line =&gt; &#123;</span><br><span class="line">      broadcast.value.contains(line)</span><br><span class="line">    &#125;).foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>广播变量只在driver端定义，不能在executor端定义。也不能在executor端进行更改。</p></blockquote><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><p>依然是driver和excutor端的数据不能共享的问题。excutor端修改了变量，根本不会让driver端跟着修改，这个就是累加器出现的原因。</p><p>累加器的作用：<br>提供了将工作节点中的值聚合到驱动器程序中的简单语法。</p><p>常用场景：<br>调试时对作业执行过程中的事件进行计数。</p><p><img src="https://img-blog.csdn.net/20180401185950994?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJvaWRfeHVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p> 累加器的用法如下所示：<br>(1)通过在driver中调用 SparkContext.accumulator(initialValue) 方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值initialValue 的类型。<br>(2)Spark闭包（函数序列化）里的excutor代码可以使用累加器的 += 方法（在Java中是 add ）增加累加器的值。<br>(3)driver程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue() ）来访问累加器的值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AccumulatorTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"accumulator"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> accumulator = sc.accumulator(<span class="number">0</span>); <span class="comment">//创建accumulator并初始化为0</span></span><br><span class="line">    <span class="keyword">val</span> linesRDD = sc.textFile(<span class="string">"./word"</span>)</span><br><span class="line">    <span class="keyword">val</span> result = linesRDD.map(s =&gt; &#123;</span><br><span class="line">      accumulator.add(<span class="number">1</span>) <span class="comment">//有一条数据就增加1s</span></span><br><span class="line">    &#125;)</span><br><span class="line">    result.collect();</span><br><span class="line">    println(<span class="string">"words lines is :"</span> + accumulator.value)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>累加器在Driver端定义赋初始值，累加器只能在Driver端读取，在Excutor端更新.</p></blockquote><p><img src="https://img-blog.csdn.net/20180401190109965?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZHJvaWRfeHVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spark共享变量&quot;&gt;&lt;a href=&quot;#spark共享变量&quot; class=&quot;headerlink&quot; title=&quot;spark共享变量&quot;&gt;&lt;/a&gt;spark共享变量&lt;/h1&gt;&lt;p&gt;spark有两大共享变量：广播变量和累加器&lt;/p&gt;
&lt;p&gt;累加器用来对信息进行聚合，</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ConcurrentHashMap原理</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/ConcurrentHashMap%E5%8E%9F%E7%90%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/ConcurrentHashMap%E5%8E%9F%E7%90%86/</id>
    <published>2022-09-23T13:00:35.000Z</published>
    <updated>2022-09-23T14:12:34.215Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ConcurrentHashMap原理"><a href="#ConcurrentHashMap原理" class="headerlink" title="ConcurrentHashMap原理"></a>ConcurrentHashMap原理</h1><h2 id="ConcurrentHashMap和HashMap以及Hashtable的区别"><a href="#ConcurrentHashMap和HashMap以及Hashtable的区别" class="headerlink" title="ConcurrentHashMap和HashMap以及Hashtable的区别"></a>ConcurrentHashMap和HashMap以及Hashtable的区别</h2><ul><li>HashMap是线程不安全的，因为HashMap中操作都没有加锁，因此在多线程环境下会导致数据覆盖之类的问题，所以，在多线程中使用HashMap是会抛出异常的。</li><li>HashTable是线程安全的,但是HashTable只是单纯的在put()方法上加上synchronized。保证插入时阻塞其他线程的插入操作。虽然安全，但因为设计简单，所以性能低下。</li><li>ConcurrentHashMap是线程安全的，ConcurrentHashMap并非锁住整个方法，而是通过原子操作和局部加锁的方法保证了多线程的线程安全，且尽可能减少了性能损耗。</li></ul><h2 id="ConcurrentHashMap原理-1"><a href="#ConcurrentHashMap原理-1" class="headerlink" title="ConcurrentHashMap原理"></a>ConcurrentHashMap原理</h2><p>它由多个 Segment 组合而成。Segment 本身就相当于一个 HashMap 对象。同 HashMap 一样，Segment 包含一个 HashEntry 数组，数组中的每一个 HashEntry 既是一个键值对，也是一个链表的头节点。<br><img src="https://img-blog.csdnimg.cn/20200807200052113.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>像这样的 Segment 对象，在 ConcurrentHashMap 集合中有多少个呢？有 2 的 N 次方个，共同保存在一个名为 segments 的数组当中。</p><p><img src="https://img-blog.csdnimg.cn/20200807200123141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>可以说，ConcurrentHashMap 是一个二级哈希表。在一个总的哈希表下面，有若干个子哈希表。</p><p><img src="https://img-blog.csdnimg.cn/20200807201722260.png" alt=""></p><p>其中，Segment是它的一个内部类，主要组成如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Segment</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">ReentrantLock</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">2249069246763182397L</span>;</span><br><span class="line"><span class="comment">// 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶</span></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">volatile</span> HashEntry&lt;K,V&gt;[] table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> count;</span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> modCount;</span><br><span class="line"><span class="keyword">transient</span> <span class="keyword">int</span> threshold;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">float</span> loadFactor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>HashEntry也是一个内部类，主要组成如下：<br><img src="https://img-blog.csdnimg.cn/20200807202323615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""><br>和 HashMap 的 Entry 基本一样，唯一的区别就是其中的核心数据如 value ，以及<strong>链表都是 volatile 修饰的，保证了获取时的可见性</strong>。</p><p>基于volatile修饰之后，数据获取效率是很高的。</p><h3 id="Put-操作"><a href="#Put-操作" class="headerlink" title="Put 操作"></a>Put 操作</h3><p><img src="https://img-blog.csdnimg.cn/20200807202542921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>首先是通过 key 定位到 Segment，之后在对应的 Segment 中进行具体的 put。<br><img src="https://img-blog.csdnimg.cn/20200807202759568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>这个put就是针对Segment的。<br>虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。</p><p><img src="https://img-blog.csdnimg.cn/20200807203115482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>1、尝试自旋获取锁。<br>2、如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。</p><p><img src="https://img-blog.csdnimg.cn/20200807203318514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>1、加锁操作；<br>2、遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。<br>3、为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。<br>4、释放锁；</p><p>Put 操作时，锁的是某个 Segment，其他线程对其他 Segment 的读写操作均不影响。因此解决了线程安全问题。</p><h3 id="Get-操作"><a href="#Get-操作" class="headerlink" title="Get 操作"></a>Get 操作</h3><p><img src="https://img-blog.csdnimg.cn/20200807203606205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""></p><p>1、Key 通过 Hash 之后定位到具体的 Segment；<br>2、再通过一次 Hash 定位到具体的元素上；<br>3、由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。</p><h2 id="JDK-1-8-的改进"><a href="#JDK-1-8-的改进" class="headerlink" title="JDK 1.8 的改进"></a>JDK 1.8 的改进</h2><ul><li>首先是结构上的变化，和 HashMap 一样，数组+链表改为数组+链表+红黑树。</li><li>HashEntry 改为 Node</li><li>Put 操作的变化<br><img src="https://img-blog.csdnimg.cn/2020080720541110.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""><br>1、根据 key 计算出 hashcode，然后开始遍历 table；<br>2、判断是否需要初始化；<br>3、f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。<br>4、如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。<br>5、如果都不满足，则利用 synchronized 锁写入数据。<br>6、如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。</li><li>Get 操作的变化<br><img src="https://img-blog.csdnimg.cn/20200807205906631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5MDUxNDEz,size_16,color_FFFFFF,t_70" alt=""><br>1、根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。<br>2、如果是红黑树那就按照树的方式获取值。<br>3、都不满足那就按照链表的方式遍历获取值。</li></ul><blockquote><p>1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。</p></blockquote><p>参考：<br><a href="https://blog.csdn.net/qq_42068856/article/details/126091526" target="_blank" rel="noopener">1.8版本ConcurrentHashMap的put原理</a></p><h2 id="面试常问的"><a href="#面试常问的" class="headerlink" title="面试常问的"></a>面试常问的</h2><h3 id="1-7-版本和-1-8-版本有啥区别，关于ConcurrentHashMap"><a href="#1-7-版本和-1-8-版本有啥区别，关于ConcurrentHashMap" class="headerlink" title="1.7 版本和 1.8 版本有啥区别，关于ConcurrentHashMap"></a>1.7 版本和 1.8 版本有啥区别，关于ConcurrentHashMap</h3><h4 id="1-7"><a href="#1-7" class="headerlink" title="1.7"></a>1.7</h4><p>JDK1.7 中的 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成，即 ConcurrentHashMap 把哈希桶数组切分成小数组（Segment ），每个小数组有 n 个 HashEntry 组成。</p><p>如下图所示，首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一段数据时，其他段的数据也能被其他线程访问，实现了真正的并发访问。<br><img src="https://img-blog.csdnimg.cn/img_convert/8bf6d4aa717a871c73d5d2699d822fed.png" alt=""></p><p>Segment 是 ConcurrentHashMap 的一个内部类，主要的组成如下：<br><img src="https://img-blog.csdnimg.cn/img_convert/892f684f7d943d1194b290b5f414d558.png" alt=""></p><p>Segment 继承了 ReentrantLock，所以 Segment 是一种可重入锁，扮演锁的角色。Segment 默认为 16，也就是并发度为 16。</p><p>存放元素的 HashEntry，也是一个静态内部类，主要的组成如下：<br><img src="https://img-blog.csdnimg.cn/img_convert/d41507b8e7a653650561367d44e5411d.png" alt=""></p><p>其中，用 volatile 修饰了 HashEntry 的数据 value 和 下一个节点 next，保证了多线程环境下数据获取时的可见性！</p><h4 id="1-8"><a href="#1-8" class="headerlink" title="1.8"></a>1.8</h4><p>JDK1.8 中的ConcurrentHashMap 选择了与 HashMap 相同的Node数组+链表+红黑树结构；在锁的实现上，抛弃了原有的 Segment 分段锁，采用CAS + synchronized实现更加细粒度的锁。<br>将锁的级别控制在了更细粒度的<strong>哈希桶数组元素级别</strong>，也就是说只需要锁住这个链表头节点（红黑树的根节点），就不会影响其他的哈希桶数组元素的读写，大大提高了并发度。<br><img src="https://img-blog.csdnimg.cn/img_convert/3e0ef497f95c88e6d409cd6f0737bfe3.png" alt=""></p><h3 id="JDK1-8-中为什么使用内置锁-synchronized替换-可重入锁-ReentrantLock？"><a href="#JDK1-8-中为什么使用内置锁-synchronized替换-可重入锁-ReentrantLock？" class="headerlink" title="JDK1.8 中为什么使用内置锁 synchronized替换 可重入锁 ReentrantLock？"></a>JDK1.8 中为什么使用内置锁 synchronized替换 可重入锁 ReentrantLock？</h3><ul><li>1.6版本针对synchronized进行了优化，他的效率高了很多。</li><li>减少内存开销 。假设使用可重入锁来获得同步支持，那么每个节点都需要通过继承 AQS 来获得同步支持。但并不是每个节点都需要获得同步支持的，只有<strong>链表的头节点（红黑树的根节点）需要同步</strong>，这无疑带来了巨大内存浪费。</li></ul><h3 id="ConcurrentHashMap-的-put-方法执行逻辑是什么？"><a href="#ConcurrentHashMap-的-put-方法执行逻辑是什么？" class="headerlink" title="ConcurrentHashMap 的 put 方法执行逻辑是什么？"></a>ConcurrentHashMap 的 put 方法执行逻辑是什么？</h3><h4 id="JDK1-7"><a href="#JDK1-7" class="headerlink" title="JDK1.7"></a>JDK1.7</h4><p><img src="https://img-blog.csdnimg.cn/img_convert/d74b564ac361f2b01f314198eb5dc8ea.png" alt=""><br>先定位到相应的 Segment ，然后再进行 put 操作。<br><img src="https://img-blog.csdnimg.cn/img_convert/76cc8ca680a660a3a04b65b29801cb0b.png" alt=""><br>首先会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。</p><ul><li>尝试自旋获取锁。</li><li>如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。</li></ul><h4 id="JDK1-8"><a href="#JDK1-8" class="headerlink" title="JDK1.8"></a>JDK1.8</h4><ul><li>根据 key 计算出 hash 值；</li><li>判断是否需要进行初始化；</li><li>定位到 Node，拿到首节点 f，判断首节点 f：<ul><li>如果为 null ，则通过 CAS 的方式尝试添加；</li><li>如果为 f.hash = MOVED = -1 ，说明其他线程在扩容，参与一起扩容；</li><li>如果都不满足 ，synchronized 锁住 f 节点，判断是链表还是红黑树，遍历插入；</li><li>当在链表长度达到 8 的时候，数组扩容或者将链表转换为红黑树。<br><img src="https://img-blog.csdnimg.cn/img_convert/d2eb3182bc96169e2f29c6df38f92891.png" alt=""></li></ul></li></ul><h3 id="ConcurrentHashMap-的-get-方法执行逻辑是什么？"><a href="#ConcurrentHashMap-的-get-方法执行逻辑是什么？" class="headerlink" title="ConcurrentHashMap 的 get 方法执行逻辑是什么？"></a>ConcurrentHashMap 的 get 方法执行逻辑是什么？</h3><h4 id="JDK1-7-1"><a href="#JDK1-7-1" class="headerlink" title="JDK1.7"></a>JDK1.7</h4><p>首先，根据 key 计算出 hash 值定位到具体的 Segment ，再根据 hash 值获取定位 HashEntry 对象，并对 HashEntry 对象进行链表遍历，找到对应元素。</p><p>由于 HashEntry 涉及到的共享变量都使用 volatile 修饰，volatile 可以保证内存可见性，所以每次获取时都是最新值。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/19e8e13c0fc91718739fbc3388c9797f.png" alt=""></p><h4 id="JDK1-8-1"><a href="#JDK1-8-1" class="headerlink" title="JDK1.8"></a>JDK1.8</h4><ul><li>根据 key 计算出 hash 值，判断数组是否为空；</li><li>如果是首节点，就直接返回；</li><li>如果是红黑树结构，就从红黑树里面查询；</li><li>如果是链表结构，循环遍历判断。<br><img src="https://img-blog.csdnimg.cn/img_convert/753d017dc01e3096673800357165e067.png" alt=""></li></ul><h3 id="ConcurrentHashMap-的-get-方法是否要加锁，为什么？"><a href="#ConcurrentHashMap-的-get-方法是否要加锁，为什么？" class="headerlink" title="ConcurrentHashMap 的 get 方法是否要加锁，为什么？"></a>ConcurrentHashMap 的 get 方法是否要加锁，为什么？</h3><p>get 方法不需要加锁。因为 Node 的元素 value 和指针 next 是用 volatile 修饰的，在多线程环境下线程A修改节点的 value 或者新增节点的时候是对线程B可见的。<br>这也是它比其他并发集合比如 Hashtable、用 Collections.synchronizedMap()包装的 HashMap 效率高的原因之一。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/7c1ca4b6190fe58a20f586164d017851.png" alt=""></p><h3 id="get-方法不需要加锁与-volatile-修饰的哈希桶数组有关吗？"><a href="#get-方法不需要加锁与-volatile-修饰的哈希桶数组有关吗？" class="headerlink" title="get 方法不需要加锁与 volatile 修饰的哈希桶数组有关吗？"></a>get 方法不需要加锁与 volatile 修饰的哈希桶数组有关吗？</h3><p>没有关系。哈希桶数组table用 volatile 修饰主要是保证在数组扩容的时候保证可见性。<br><img src="https://img-blog.csdnimg.cn/img_convert/3f10cd575e3750a4e384f71fd7adc3cc.png" alt=""></p><h3 id="ConcurrentHashMap-不支持-key-或者-value-为-null-的原因？"><a href="#ConcurrentHashMap-不支持-key-或者-value-为-null-的原因？" class="headerlink" title="ConcurrentHashMap 不支持 key 或者 value 为 null 的原因？"></a>ConcurrentHashMap 不支持 key 或者 value 为 null 的原因？</h3><p>我们先来说value 为什么不能为 null。因为 ConcurrentHashMap 是用于多线程的 ，如果ConcurrentHashMap.get(key)得到了 null ，这就无法判断，是映射的value是 null ，还是没有找到对应的key而为 null ，就有了<strong>二义性</strong>。</p><p>而用于单线程状态的 HashMap 却可以用<strong>containsKey(key)</strong> 去判断到底是否包含了这个 null 。</p><h3 id="ConcurrentHashMap-的并发度是什么？"><a href="#ConcurrentHashMap-的并发度是什么？" class="headerlink" title="ConcurrentHashMap 的并发度是什么？"></a>ConcurrentHashMap 的并发度是什么？</h3><p>并发度可以理解为程序运行时能够同时更新 ConccurentHashMap且不产生锁竞争的最大线程数。在JDK1.7中，实际上就是ConcurrentHashMap中的分段锁个数，即Segment[]的数组长度，默认是16，这个值可以在构造函数中设置。<br>如果自己设置了并发度，ConcurrentHashMap 会使用大于等于该值的最小的2的幂指数作为实际并发度，也就是比如你设置的值是17，那么实际并发度是32。<br>如果并发度设置的过小，会带来严重的锁竞争问题；如果并发度设置的过大，原本位于同一个Segment内的访问会扩散到不同的Segment中，CPU cache命中率会下降，从而引起程序性能下降。</p><p>在JDK1.8中，已经摒弃了Segment的概念，选择了Node数组+链表+红黑树结构，并发度大小依赖于数组的大小。</p><h3 id="ConcurrentHashMap-迭代器是强一致性还是弱一致性？"><a href="#ConcurrentHashMap-迭代器是强一致性还是弱一致性？" class="headerlink" title="ConcurrentHashMap 迭代器是强一致性还是弱一致性？"></a>ConcurrentHashMap 迭代器是强一致性还是弱一致性？</h3><p>与 HashMap 迭代器是强一致性不同，ConcurrentHashMap 迭代器是弱一致性。<br>ConcurrentHashMap 的迭代器创建后，就会按照哈希表结构遍历每个元素，但在遍历过程中，内部元素可能会发生变化，如果变化发生在已遍历过的部分，迭代器就不会反映出来，而如果变化发生在未遍历过的部分，迭代器就会发现并反映出来，这就是弱一致性。<br>这样迭代器线程可以使用原来老的数据，而写线程也可以并发的完成改变，更重要的，这保证了多个线程并发执行的连续性和扩展性，是性能提升的关键。</p><h3 id="ConcurrentHashMap-和-Hashtable-的效率哪个更高？为什么？"><a href="#ConcurrentHashMap-和-Hashtable-的效率哪个更高？为什么？" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的效率哪个更高？为什么？"></a>ConcurrentHashMap 和 Hashtable 的效率哪个更高？为什么？</h3><p>ConcurrentHashMap 的效率要高于 Hashtable，因为 Hashtable 给整个哈希表加了一把大锁从而实现线程安全。而ConcurrentHashMap 的锁粒度更低，在 JDK1.7 中采用分段锁实现线程安全，在 JDK1.8 中采用CAS+synchronized实现线程安全。</p><h3 id="具体说一下Hashtable的锁机制"><a href="#具体说一下Hashtable的锁机制" class="headerlink" title="具体说一下Hashtable的锁机制"></a>具体说一下Hashtable的锁机制</h3><p>Hashtable 是使用 synchronized来实现线程安全的，给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞等待需要的锁被释放，在竞争激烈的多线程场景中性能就会非常差！</p><h3 id="多线程下安全的操作-map还有其他方法吗？"><a href="#多线程下安全的操作-map还有其他方法吗？" class="headerlink" title="多线程下安全的操作 map还有其他方法吗？"></a>多线程下安全的操作 map还有其他方法吗？</h3><p>还可以使用Collections.synchronizedMap方法，对方法进行加同步锁。<br><img src="https://img-blog.csdnimg.cn/img_convert/1e537606d716a73c972b0ebf7aa10da6.png" alt=""><br>如果传入的是 HashMap 对象，其实也是对 HashMap 做的方法做了一层包装，里面使用对象锁来保证多线程场景下，线程安全，本质也是对 HashMap 进行<strong>全表锁</strong>。在竞争激烈的多线程环境下性能依然也非常差，不推荐使用！</p><blockquote><p>补记：jdk 1.8 的put操作<br>做插入操作时，首先进入乐观锁，<br>然后，在乐观锁中判断容器是否初始化，<br>如果没初始化则初始化容器，<br>如果已经初始化，则判断该hash位置的节点是否为空，如果为空，则通过CAS操作进行插入。<br>如果该节点不为空，再判断容器是否在扩容中，如果在扩容，则帮助其扩容。<br>如果没有扩容，则进行最后一步，先加锁，然后找到hash值相同的那个节点(hash冲突)，<br>循环判断这个节点上的链表，决定做覆盖操作还是插入操作。<br>循环结束，插入完毕。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;ConcurrentHashMap原理&quot;&gt;&lt;a href=&quot;#ConcurrentHashMap原理&quot; class=&quot;headerlink&quot; title=&quot;ConcurrentHashMap原理&quot;&gt;&lt;/a&gt;ConcurrentHashMap原理&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>hashmap原理</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/hashmap%E5%8E%9F%E7%90%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/hashmap%E5%8E%9F%E7%90%86/</id>
    <published>2022-09-23T08:48:09.000Z</published>
    <updated>2022-09-23T13:07:55.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hashmap原理"><a href="#hashmap原理" class="headerlink" title="hashmap原理"></a>hashmap原理</h1><p>hashmap是java中最常用，java优化也是比较多。想用好，先学好。</p><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>HashMap是:数组+链表+红黑树。<br><img src="https://pics0.baidu.com/feed/b3fb43166d224f4a6a0d7e072f3a015b9922d160.png@f_auto?token=29c0e47830005f8f6e8eec430255ddd7" alt=""></p><h3 id="核心成员"><a href="#核心成员" class="headerlink" title="核心成员"></a>核心成员</h3><ul><li>默认初始容量(数组默认大小):16，2的整数次方 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;   </li><li>最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; </li><li>默认负载因子 static final float DEFAULT_LOAD_FACTOR = 0.75f;装载因子用来衡量HashMap满的程度，表示当map集合中存储的数据达到当前数组大小的75%则需要进行扩容 </li><li>链表转红黑树边界static final int TREEIFY_THRESHOLD = 8; </li><li>红黑树转离链表边界static final int UNTREEIFY_THRESHOLD = 6;</li><li>哈希桶数组 transient Node&lt;K,V&gt;[] table; </li><li>实际存储的元素个数transient int size; </li><li>当map里面的数据大于这个threshold就会进行扩容 int threshold   阈值 = table.length * loadFactor</li></ul><h3 id="Node数组"><a href="#Node数组" class="headerlink" title="Node数组"></a>Node数组</h3><p>从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;    </span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> hash;<span class="comment">//用来定位数组索引位置    </span></span><br><span class="line">    <span class="keyword">final</span> K key;V value;</span><br><span class="line">    Node&lt;K,V&gt; next;<span class="comment">//链表的下一个Node节点    </span></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;        </span><br><span class="line">        <span class="keyword">this</span>.hash = hash;        </span><br><span class="line">        <span class="keyword">this</span>.key = key;        </span><br><span class="line">        <span class="keyword">this</span>.value = value;        </span><br><span class="line">        <span class="keyword">this</span>.next = next;    </span><br><span class="line">    &#125;      </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span></span>&#123; </span><br><span class="line">        <span class="keyword">return</span> key; </span><br><span class="line">    &#125;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span></span>&#123; </span><br><span class="line">        <span class="keyword">return</span> value; </span><br><span class="line">    &#125;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; </span><br><span class="line">        <span class="keyword">return</span> key + <span class="string">"="</span> + value; </span><br><span class="line">    &#125;      </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;        </span><br><span class="line">        <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);    </span><br><span class="line">    &#125;      </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;</span><br><span class="line">        V oldValue = value;        </span><br><span class="line">        value = newValue;        </span><br><span class="line">        <span class="keyword">return</span> oldValue;    </span><br><span class="line">    &#125;      </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;        </span><br><span class="line">        <span class="keyword">if</span> (o == <span class="keyword">this</span>)<span class="keyword">return</span> <span class="keyword">true</span>;        </span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> Map.Entry) &#123;            </span><br><span class="line">            Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;           </span><br><span class="line">            <span class="keyword">if</span> (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue()))<span class="keyword">return</span> <span class="keyword">true</span>;        </span><br><span class="line">        &#125;        </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。</p><h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p>HashMap采用哈希表来存储数据。<br>哈希表（Hash table，也叫散列表），是根据关键码值(Key value)而直接进行访问的数据结构，只要输入待查找的值即key，即可查找到其对应的值。<br><strong>哈希表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表</strong>。</p><p>哈希表中元素是由哈希函数确定的,将数据元素的关键字Key作为自变量，通过一定的函数关系（称为哈希函数），计算出的值，即为该元素的存储地址。<br>表示为：Addr = H（key）,如下图所示：<br><img src="https://pics5.baidu.com/feed/18d8bc3eb13533faef0c2bf48c1e6c1643345b8c.png@f_auto?token=f0af5565175329bc84dabc2d34126878" alt=""></p><p>有hash就会有冲突，所以如何处理冲突就成为一个核心问题。</p><h3 id="链式哈希表"><a href="#链式哈希表" class="headerlink" title="链式哈希表"></a>链式哈希表</h3><p>哈希表为解决冲突，可以采用地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。<br><img src="https://pics7.baidu.com/feed/2f738bd4b31c8701d86059270cb20f260508ff70.jpeg@f_auto?token=fb1a69d6611c47b0c838c41bed5faa5c" alt=""></p><h4 id="hash函数"><a href="#hash函数" class="headerlink" title="hash函数"></a>hash函数</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*** 重新计算哈希值*/</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;        </span><br><span class="line">    <span class="keyword">int</span> h;      </span><br><span class="line">    <span class="comment">// h = key.hashCode() 为第一步 取hashCode值     </span></span><br><span class="line">    <span class="comment">// h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 </span></span><br><span class="line">    <span class="keyword">return</span> (key == <span class="keyword">null</span>) ? <span class="number">0</span> : (h = key.hashCode()) ^ (h &gt;&gt;&gt; <span class="number">16</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>计算数组槽位</p><blockquote><p>(n - 1) &amp; hash</p></blockquote><p>对key进行了hashCode运算，得到一个32位的int值h,然后用h 异或 h&gt;&gt;&gt;16位。在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)。<br><img src="https://pics7.baidu.com/feed/d0c8a786c9177f3e7b0214b04202aace9e3d56ab.jpeg@f_auto?token=5c8de27e8ac0b0f17234109defcba69c" alt=""></p><p>这样做的好处是，可以将hashcode高位和低位的值进行混合做异或运算，而且混合后，低位的信息中加入了高位的信息，这样高位的信息被变相地保留了下来。</p><p>等于说计算下标时把hash的高16位也参与进来了，<strong>掺杂的元素多了</strong>，那么生成的hash值的随机性会增大，减少了hash碰撞。</p><p>h &amp; (table.length -1)来得到该对象的保存位，也就是数组的下标，而HashMap底层数组的长度总是2的n次方。</p><h4 id="为什么槽位数必须使用2-n？"><a href="#为什么槽位数必须使用2-n？" class="headerlink" title="为什么槽位数必须使用2^n？"></a>为什么槽位数必须使用2^n？</h4><p><strong>为了让哈希后的结果更加均匀。</strong><br><img src="https://pics2.baidu.com/feed/7e3e6709c93d70cf57c609b2d4114009bba12ba3.png@f_auto?token=c683ef70b4c7d40c26eb89f39aa65855" alt=""></p><p>假如槽位数不是16，而是17，则槽位计算公式变成：(17 – 1) &amp; hash<br>从上文可以看出，计算结果将会大大趋同，hashcode参加&amp;运算后被更多位的0屏蔽，计算结果只剩下两种0和16，这对于hashmap来说是一种灾难。<br>2.当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。</p><blockquote><p>位运算的运算效率高于算术运算，原因是算术运算还是会被转化为位运算。</p></blockquote><h3 id="HashMap的put方法"><a href="#HashMap的put方法" class="headerlink" title="HashMap的put方法"></a>HashMap的put方法</h3><p>①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容；</p><p>②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加</p><p>③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value</p><p>④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对</p><p>⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</p><p>⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</p><blockquote><p>HashMap中数组长度的原始大小为16，并不是上面展示的长度，且数组的初始值都为null。</p></blockquote><p>链表的插入，在1.7版本的java，用的是头插法，会存在死循环问题。1.8版本已经迭代为尾插法。</p><blockquote><p>在Node的元素匹配中，是依靠key值的equals方法来比对的。</p></blockquote><h2 id="HashMap和Hashtable的区别"><a href="#HashMap和Hashtable的区别" class="headerlink" title="HashMap和Hashtable的区别"></a>HashMap和Hashtable的区别</h2><ol><li>底层数据结构不同:jdk1.7底层都是数组+链表,但jdk1.8 HashMap加入了红黑树</li><li>Hashtable 是不允许键或值为 null 的，HashMap 的键值则都可以为 null。</li><li>添加key-value的hash值算法不同：HashMap添加元素时，是使用自定义的哈希算法,而HashTable是直接采用key的hashCode()</li><li>实现方式不同：Hashtable 继承的是 Dictionary类，而 HashMap 继承的是 AbstractMap 类。</li><li>初始化容量不同：HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75。</li><li>扩容机制不同：当已用容量&gt;总容量 * 负载因子时，HashMap 扩容规则为当前容量翻倍，Hashtable 扩容规则为当前容量翻倍 +1。</li><li>支持的遍历种类不同：HashMap只支持Iterator遍历,而HashTable支持Iterator和Enumeration两种方式遍历</li><li>迭代器不同：HashMap的迭代器(Iterator)是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。而Hashtable 则不会。</li><li>部分API不同：HashMap不支持contains(Object value)方法，没有重写toString()方法,而HashTable支持contains(Object value)方法，而且重写了toString()方法</li><li>同步性不同: Hashtable是同步(synchronized)的，适用于多线程环境,而hashmap不是同步的，适用于单线程环境。多个线程可以共享一个Hashtable；而如果没有正确的同步的话，多个线程是不能共享HashMap的。</li></ol><blockquote><p>由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;hashmap原理&quot;&gt;&lt;a href=&quot;#hashmap原理&quot; class=&quot;headerlink&quot; title=&quot;hashmap原理&quot;&gt;&lt;/a&gt;hashmap原理&lt;/h1&gt;&lt;p&gt;hashmap是java中最常用，java优化也是比较多。想用好，先学好。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>cpu多级缓存原理</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/cpu%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E5%8E%9F%E7%90%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/cpu%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%E5%8E%9F%E7%90%86/</id>
    <published>2022-09-23T08:27:54.000Z</published>
    <updated>2022-09-23T08:47:58.166Z</updated>
    
    
    
    
    <category term="基础" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>线程切换</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/%E7%BA%BF%E7%A8%8B%E5%88%87%E6%8D%A2/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/%E7%BA%BF%E7%A8%8B%E5%88%87%E6%8D%A2/</id>
    <published>2022-09-23T08:27:27.000Z</published>
    <updated>2022-09-23T08:46:31.229Z</updated>
    
    
    
    
    <category term="基础" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%9F%BA%E7%A1%80/"/>
    
    
  </entry>
  
  <entry>
    <title>java内存模型</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-09-23T01:30:54.000Z</published>
    <updated>2022-10-12T12:03:37.670Z</updated>
    
    <content type="html"><![CDATA[<h1 id="java内存模型"><a href="#java内存模型" class="headerlink" title="java内存模型"></a>java内存模型</h1><p>Java 内存模型定义了 Java 语言如何与内存进行交互，具体地说是 Java 语言运行时的变量，如何与我们的硬件内存进行交互的。</p><blockquote><p>注意与jvm内存模型的区分</p></blockquote><p>Java 内存模型是并发编程的基础，只有对 Java 内存模型理解较为透彻，我们才能避免一些错误地理解。Java 中一些高级的特性，也建立在 Java 内存模型的基础上，例如：volatile 关键字。<br>Java 内存模型的本身的发展也是跟计算发展息息相关的，在计算机语言发展的历史中，不同语言对于这部分内容的态度是不一样的，因为这块内容是牵扯计算机操作系统和语言本身的一个环节。<br>带着以下三个问题来看下边的内容：</p><ul><li>为什么要有 Java 内存模型？</li><li>Java 内存模型解决了什么问题？</li><li>Java 内存模型是怎样的一个东西？</li></ul><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><p>cpu和内存是计算机的两大组件，cpu负责计算，内存负责存储数据，但是cpu的计算速度远远大于从内存获取数据的速度，因此也就会出现cpu等待的现象。<br>由于两者的速度差距实在太大，我们为了加快运行速度，于是计算机的设计者在 CPU 中加了一个CPU 高速缓存。这个 CPU 高速缓存的速度介于 CPU 与内存之间，每次需要读取数据的时候，先从内存读取到CPU缓存中，CPU再从CPU缓存中读取。这样虽然还是存在速度差异，但至少不像之前差距那么大了。</p><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/%E6%96%B0%E5%A2%9E%20CPU%20%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98.png" alt=""></p><p>随着技术的发展，多核 CPU 出现了，CPU 的计算能力进一步提高。原本同一时间只能运行一个任务，但现在可以同时运行多个任务。由于多核 CPU 的出现，虽然提高了 CPU 的处理速度，但也带来了新的问题：缓存一致性。<br>在多 CPU 系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存，如下图所示。当多个 CPU 的运算任务都涉及同一块主内存区域时，可能导致各自的缓存数据不一致。如果发生了这种情况，那同步回主内存时以哪个 CPU 高速缓存的数据为准呢？</p><p>这是在并行领域必然遇到的问题。</p><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/%E5%A4%9A%E6%A0%B8%20CPU%20%E5%8F%8A%E9%AB%98%E9%80%9F%E7%BC%93%E5%AD%98%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98.png" alt=""></p><p>到这里，就产生了第一个问题：硬件层面上，由于多 CPU 的存在，以及加入 CPU 高速缓存，导致的数据一致性问题。</p><p>要注意的是，这个问题是硬件层面上的问题。只要使用了多 CPU 并且 CPU 有高速缓存，那就会遇到这个问题。对于生产该 CPU 的厂商，就需要去解决这个问题，这与具体操作系统无关，也与编程语言无关。</p><p>那么如何解决这个问题呢？答案是：缓存一致性协议。</p><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/%E5%8A%A0%E5%85%A5%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AE.png" alt=""></p><p>所谓的缓存一致性协议，指的是在 CPU 高速缓存与主内存交互的时候，遵守特定的规则，这样就可以避免数据一致性问题了。</p><p>在不同的 CPU 中，会使用不同的缓存一致性协议。例如 MESI 协议用于奔腾系列的 CPU 中，而 MOSEI 协议则用于 AMD 系列 CPU 中，Intel 的 core i7 处理器使用 MESIF 协议。在这里我们介绍最为常见的一种：MESI数据一致性协议。</p><p>在 MESI 协议中，每个缓存可能有有4个状态，它们分别是：</p><ul><li>M(Modified)：这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本 Cache 中。</li><li>E(Exclusive)：这行数据有效，数据和内存中的数据一致，数据只存在于本 Cache 中。</li><li>S(Shared)：这行数据有效，数据和内存中的数据一致，数据存在于很多 Cache 中。</li><li>I(Invalid)：这行数据无效。</li></ul><p>在数据回写到主内存的时候，会把其他cpu高速缓存中，该变量的值设置为Invalid，这样其他CPU从自己的缓存中读取到该变量的时候发现是Invalid状态，就会去主内存中获取数据。</p><blockquote><p>MESI 缓存一致性协议，本质上是定义了一些内存状态，然后通过消息的方式通知其他 CPU 高速缓存，从而解决了数据一致性的问题。</p></blockquote><h2 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h2><p>操作系统上层语言跟计算机硬件交互的桥梁，很多硬件资源都被虚拟化成另一个比较好理解的概念，让上层更容易交互。比如用户态、内核态、cpu时间片等。</p><p>前面说到 CPU 与内存之间会存在缓存一致性问题，那操作系统抽象出来的 CPU 与内存也会面临这样的问题。因此，操作系统层面也需要去解决同样的问题。所以，对于任何一个系统来说，它们都需要去解决这样一个问题。<strong>我们把在特定的操作协议下，对特定内存或高速缓存进行读写访问的过程进行抽象，得到的就是内存模型了</strong>。 无论是 Windows 系统，还是 Linux 系统，它们都有特定的内存模型。</p><p>Java 语言是建立在操作系统上层的高级语言，它只能与操作系统进行交互，而不与硬件进行交互。与操作系统相对于硬件类似，操作系统需要抽象出内存模型，那么 Java 语言也需要抽象出相对于操作系统的内存模型。一般来说，编程语言也可以直接复用操作系统层面的内存模型，例如：C++ 语言就是这么做的。但由于不同操作系统的内存模型不同，有可能导致程序在一套平台上并发完全正常，而在另外一套平台上并发访问却经常出错。因此在某些场景下，就必须针对不同的平台来编写程序。</p><p>而我们都知道 Java 的最大特点是「Write Once, Run Anywhere」，即一次编译哪里都可以运行。而为了达到这样一个目标，Java 语言就必须在各个操作系统的基础上进一步抽象，建立起一套对内存或高速缓存的读写访问抽象标准。这样就可以保证无论在哪个操作系统，只要遵循了这个规范，都能保证并发访问是正常的。</p><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%20-%20%E4%B8%8D%E5%90%8C%E5%B1%82%E9%9D%A2%E6%8A%BD%E8%B1%A1%E5%8F%8A%E6%96%B9%E6%A1%88.png" alt=""></p><h2 id="Java内存模型"><a href="#Java内存模型" class="headerlink" title="Java内存模型"></a>Java内存模型</h2><p><strong>Java 内存模型（Java Memory Model，JMM）用于屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台都能达到一致的内存访问效果。</strong></p><p>Java 内存模型定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。这里说的变量包括了实例字段、静态字段和构成数组对象的元素，但不包括局部变量与方法参数。因为后者是线程私有的，不会被共享，自然就不会存在竞争问题。</p><h3 id="内存模型的定义"><a href="#内存模型的定义" class="headerlink" title="内存模型的定义"></a>内存模型的定义</h3><p>Java 内存模型规定所有的变量都存储在主内存中，每条线程都有自己的工作内存。线程的工作内存中保存了被该线程使用到的变量的主内存副本拷贝，线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递都需要通过主内存来完成。主内存、工作内存、线程三者之间的关系如下图所示。</p><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/Java%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%9B%BE%E8%A7%A3.png" alt=""></p><p>Java 内存模型的主内存、工作内存与 JVM 的堆、栈、方法区，并不是同一层次的内存划分，两者是没有关联的。如果一定要对应一下，那么主内存主要对应于 Java 堆中对象实例的数据部分，而工作内存则对应于虚拟机栈中的部分区域。</p><h3 id="内存间的交互"><a href="#内存间的交互" class="headerlink" title="内存间的交互"></a>内存间的交互</h3><p>关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存，以及如何从工作内存同步回主内存的细节，Java 内存模型定义了 8 种操作来完成。虚拟机实现的时候必须保证下面提及的每一种操作都是原子的、不可再分的。</p><ul><li>lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。</li><li>unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。</li><li>read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。</li><li>load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。</li><li>use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作。</li><li>assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。</li><li>store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用。</li><li>write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。</li></ul><p>如果要把一个变量从主内存复制到工作内存，那就要顺序地执行 read 和 load 操作，如果要把变量从工作内存同步回主内存，就要顺序地执行 store 和 write 操作。注意，<strong>Java 内存模型只要求上述两个操作必须按顺序执行，而没有保证是连续执行</strong>。也就是说，read 与 load 之间、store 与 write 之间是可插入其他指令的，如对主内存中的变量 a、b 进行访问时，一种可能出现顺序是 read a、read b、load b、load a。</p><blockquote><p>通过以上操作其实可以推测出操作系统本身也是按照这个逻辑在操作的，而且有自己的优化在里边，比如回写主内存，实际上是依赖于操作系统自己的调度。</p></blockquote><p>此外，Java 内存模型还规定上述 8 种基本操作时必须满足如下规则：</p><ul><li>不允许read和load、store和write操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者从工作内存发起回写了但主内存不接受的情况出现。</li><li>不允许一个线程丢弃它的最近的 assign 操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。</li><li>不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。</li><li>一个新的变量只能在主内存中「诞生」，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说，就是对一个变量实施use、store操作之前，必须先执行过了assign和load操作。</li><li>一个变量在同一个时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。</li><li>如果对一个变量执行lock操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。</li><li>如果一个变量事先没有被lock操作锁定，那就不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定住的变量。</li><li>对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）。</li></ul><blockquote><p>这里对于lock和unlock的操作，都涉及数据的重新读取以及必须写回主内存的一些约定。这就对于一些消息机制有影响了，让lock操作看起来就是一个消息传递或者数据共享的过程。</p></blockquote><p>这 8 种内存访问操作以及上述规则限定，再加上对 volatile 的一些特殊规定，就已经完全确定了 Java 程序中哪些内存访问操作在并发下是安全的。 看完了 Java 内存模型的 8 个基本操作和 8 个规则，感觉太过于繁琐了，非常不利于我们日常代码的编写。为了能帮助编程人员理解，于是就有了与其相等价的判断原则 —— happen-before 原则，它可以用于判断一个访问在并发环境下是否安全。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>由于多核 CPU 和高速缓存在存在，导致了缓存一致性问题。这个问题属于硬件层面上的问题，而解决办法是各种缓存一致性协议。不同 CPU 采用的协议不同，MESI 是最经典的一个缓存一致性协议。</li><li>操作系统作为对底层硬件的抽象，自然也需要解决 CPU 高速缓存与内存之间的缓存一致性问题。各个操作系统都对 CPU 高速缓存与缓存的读写访问过程进行抽象，最终得到的一个东西就是「内存模型」。</li><li>Java 语言作为运行在操作系统层面的高级语言，为了解决多平台运行的问题，在操作系统基础上进一步抽象，得到了 Java 语言层面上的内存模型。</li><li>Java 内存模型分为工作内存与主内存，每个线程都有自己的工作内存。每个线程都不能直接与主内存交互，只能与工作内存交互。此外，为了保证并发编程下的数据准确性，Java 内存模型还定义了 8 个基本的原子操作，以及 8 条基本的规则。</li></ol><blockquote><p>如果 Java 程序能够遵守 Java 内存模型的规则，那么其写出的程序就是并发安全的，这就是 Java 内存模型最大的价值。</p></blockquote><p><img src="https://shuyi-tech-blog.oss-cn-shenzhen.aliyuncs.com/halo_blog_system_file/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%20Java%20%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.png" alt=""></p><p>参考：<br><a href="https://www.cnblogs.com/chanshuyi/p/deep-insight-of-java-memory-model.html" target="_blank" rel="noopener">https://www.cnblogs.com/chanshuyi/p/deep-insight-of-java-memory-model.html</a></p><p>学习：<br><a href="https://www.zhihu.com/question/296949412/answer/747494794" target="_blank" rel="noopener">有了MESI,为什么还需要volatile</a></p><p>上边那个问题的答案我单独记录以下：</p><ol><li>volatile是用户可以对java编译器进行指令重排序的一种干预。</li><li>MESI只是保证了多核cpu的独占cache之间的一致性，但是cpu并不是直接把数据写入L1 cache的，中间还可能有store buffer。有些arm和power架构的cpu还可能有load buffer或者invalid queue等等。也就是说MESI只是决定了回写，但是并不保证会立马写入主内存。而volatile可以有这样的能力。</li><li>MSEI协议这里保证的仅仅coherence而不是consistency，而volatile可以两者都保证。<blockquote><p>Coherence deals with maintaining a global order in which writes to a single location or single variable are seen by all processors. Consistency deals with the ordering of operations to multiple locations with respect to all processors.<br>Coherence处理的是维护一个全局顺序，在这个全局顺序中，所有处理器都可以看到对单个位置或单个变量的写入。Consistency处理相对于所有处理器的多个位置的操作顺序。<br>因此，MESI协议最多只是保证了对于一个变量，在多个核上的读写顺序，对于多个变量而言是没有任何保证的。</p></blockquote></li><li>对于arm和power这个weak consistency[3]的架构的cpu来说，它们只会保证指令之间有比如控制依赖，数据依赖，地址依赖等等依赖关系的指令间提交的先后顺序，而对于完全没有依赖关系的指令，比如x=1;y=2，它们是不会保证执行提交的顺序的，除非你使用了volatile，java把volatile编译成arm和power能够识别的barrier指令，这个时候才是按顺序的。</li></ol><p><strong>Lock前缀的指令会强制写入主内存，也可以避免前后指令的重排序，并及时让其他核的相应缓存行失效，从而利用MESI达到符合预期的效果。</strong></p><p>volatile是一个高层的表达意图的“抽象”，而MESI是为了实现这个抽象，在某种特定情况下需要使用的一个实现细节。可以把JSR-133看作是一套UT的规范。不管底下CPU/编译器怎么折腾，只要voltile修饰的变量满足JSR-133所描述的所有场景，就算是一个好的java实现。而基于这个规范，java开发人员才能安心的开发并发代码，而不至于被底层细节搞疯。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;java内存模型&quot;&gt;&lt;a href=&quot;#java内存模型&quot; class=&quot;headerlink&quot; title=&quot;java内存模型&quot;&gt;&lt;/a&gt;java内存模型&lt;/h1&gt;&lt;p&gt;Java 内存模型定义了 Java 语言如何与内存进行交互，具体地说是 Java 语言运行时</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
    <category term="内存模型" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>垃圾回收之引用计数算法和可达性分析算法</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E7%AE%97%E6%B3%95%E5%92%8C%E5%8F%AF%E8%BE%BE%E6%80%A7%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/23/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B9%8B%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E7%AE%97%E6%B3%95%E5%92%8C%E5%8F%AF%E8%BE%BE%E6%80%A7%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/</id>
    <published>2022-09-23T00:55:35.000Z</published>
    <updated>2022-10-08T07:33:06.261Z</updated>
    
    <content type="html"><![CDATA[<h1 id="垃圾回收之引用计数算法和可达性分析算法"><a href="#垃圾回收之引用计数算法和可达性分析算法" class="headerlink" title="垃圾回收之引用计数算法和可达性分析算法"></a>垃圾回收之引用计数算法和可达性分析算法</h1><h2 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h2><p>在java中是通过引用来和对象进行关联的，也就是说如果要操作对象，必须通过引用来进行。那么很显然一个简单的办法就是通过引用计数来判断一个对象是否可以被回收。如果一个对象没有任何引用与之关联，则说明该对象基本不太可能在其他地方被使用到，那么这个对象就成为可被回收的对象了。这种方式成为引用计数法。</p><p>什么是引用计数算法：给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值加１；当引用失效时，计数器值减１,引用数量为0的时候,则说明对象没有被任何引用指向,可以认定是”垃圾”对象</p><p>这种方法实现比较简单,且效率很高,但是无法解决循环引用的问题,因此在java中没有采用此算法.</p><blockquote><p>python 和 php 都是采用的此算法</p></blockquote><p>来分析一下为什么会产生循环引用的问题,且注意看图中的注释：<br><img src="/images/1leude29yj.png" alt=""></p><p>原理如下:<br>第一步:创建A对象,存储在堆空间中,但是a变量是存储在栈帧里面的局部变量表中,所以a的引用地址就是堆空间引用地址<br>第二步:创建B对象,存储在堆空间中,但是b变量也是存储在栈帧里面的局部变量表中,所以b的引用地址就是堆空间引用地址<br>第三步:A对象的属性object的引用地址指向了B对象的引用地址<br>第四步:B对象的属性object的引用地址也执行了A对象的引用地址<br>代码图中的第五步:局部变量表中的a变量引用地址置为null,直接将下图中的第一步去掉了<br>代码图中的第六步:局部变量表中的b变量引用地址置为null,直接将下图中的第二步去掉了<br>这样就导致了堆空间中的循环相互引用的问题</p><p><img src="/images/vj5hfsanr.png" alt=""></p><h2 id="可达性分析算法（有称之-根搜索算法）"><a href="#可达性分析算法（有称之-根搜索算法）" class="headerlink" title="可达性分析算法（有称之:根搜索算法）"></a>可达性分析算法（有称之:根搜索算法）</h2><p>可达性分析算法的基本思路就是通过一系列名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为<strong>引用链(Reference Chain)</strong>，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。<br>这个算法的基本思想是通过一系列称为“GC Roots”的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链（即GC Roots到对象不可达）时，则证明此对象是不可用的。</p><p>在Java语言中,可作为GCRoots对象包含为以下几种:</p><ul><li>虚拟机栈(栈帧中的本地变量表)中引用的对象。(可以理解为:引用栈帧中的本地变量表的所有对象)</li><li>方法区中静态属性引用的对象(可以理解为:引用方法区该静态属性的所有对象)</li><li>方法区中常量引用的对象(可以理解为:引用方法区中常量的所有对象)</li><li>本地方法栈中(Native方法)引用的对象(可以理解为:引用Native方法的所有对象)</li></ul><p>理解：<br>(1)首先第一种是<strong>虚拟机栈中的引用的对象</strong>，我们在程序中正常创建一个对象，对象会在堆上开辟一块空间，同时会将这块空间的地址作为引用保存到虚拟机栈中，如果对象生命周期结束了，那么引用就会从虚拟机栈中出栈，因此如果在虚拟机栈中有引用，就说明这个对象还是有用的，这种情况是最常见的。<br>(2)第二种是我们在类中定义了<strong>全局的静态的对象</strong>，也就是使用了static关键字，由于虚拟机栈是线程私有的，所以这种对象的引用会保存在共有的方法区中，显然将方法区中的静态引用作为GC Roots是必须的。<br>(3)第三种便是常量引用，就是使用了<strong>static final关键字</strong>，由于这种引用初始化之后不会修改，所以方法区常量池里的引用的对象也应该作为GC Roots。<br>(4)最后一种是在使用<strong>JNI技术</strong>时，有时候单纯的Java代码并不能满足我们的需求，我们可能需要在Java中调用C或C++的代码，因此会使用native方法，JVM内存中专门有一块本地方法栈，用来保存这些对象的引用，所以本地方法栈中引用的对象也会被作为GC Roots。</p><p><img src="/images/b1leejjzw6.png" alt=""></p><p><img src="/images/bpd0j0xoxn.png" alt=""></p><blockquote><p> 即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程:如果对象在进行可达性分析后发现没有与GC Roots 相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize() 方法。当对象没有覆盖finalize() 方法，或者finalize() 方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。<br>如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会放置在一个叫做F-Queuc的队列之中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束，这样做的原因是如果一个对象在finalizeO 方法中执行缓慢，或者发生了死循环(更极端的情况)，将很可能会导致F-Queue队列中其他对象永久处于等待，甚至导致整个内存回收系统崩溃。finalize() 方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-QUCUC中的对象进行第二次小规模的标记，如果对象要在finalize()中成功扬救自己一次，要重新与引用链上的任何一个对象建立关联即可，譬如把自己(this 关键字) 赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合，如果对象这时候还没有逃脱，那基本上它就真的被回收了。从代码清单3-2 中我们可以看到一个对象的finalize()被执行，但是它仍然可以存活。代码清单3-2一次对象自我拯救的演示。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ccc;</span><br><span class="line"><span class="comment">/*此代码演示了两点</span></span><br><span class="line"><span class="comment"> * 对象可以在GC时自我拯救</span></span><br><span class="line"><span class="comment"> * 这种自救只会有一次，因为一个对象的finalize方法只会被自动调用一次</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FinalizeEscapeGC</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> FinalizeEscapeGC SAVE_HOOK=<span class="keyword">null</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">isAlive</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"yes我还活着"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">finalize</span><span class="params">()</span> <span class="keyword">throws</span> Throwable</span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.finalize();</span><br><span class="line">        System.out.println(<span class="string">"执行finalize方法"</span>);</span><br><span class="line">        FinalizeEscapeGC.SAVE_HOOK=<span class="keyword">this</span>;<span class="comment">//自救</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException</span>&#123;</span><br><span class="line">        SAVE_HOOK=<span class="keyword">new</span> FinalizeEscapeGC();</span><br><span class="line">        <span class="comment">//对象的第一次回收</span></span><br><span class="line">        SAVE_HOOK=<span class="keyword">null</span>;</span><br><span class="line">        System.gc();</span><br><span class="line">        <span class="comment">//因为finalize方法的优先级很低所以暂停0.5秒等它</span></span><br><span class="line">        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        <span class="keyword">if</span>(SAVE_HOOK!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            SAVE_HOOK.isAlive();</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"no我死了"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//下面的代码和上面的一样，但是这次自救却失败了</span></span><br><span class="line">        <span class="comment">//对象的第一次回收</span></span><br><span class="line">        SAVE_HOOK=<span class="keyword">null</span>;</span><br><span class="line">        System.gc();</span><br><span class="line">        Thread.sleep(<span class="number">500</span>);</span><br><span class="line">        <span class="keyword">if</span>(SAVE_HOOK!=<span class="keyword">null</span>)&#123;</span><br><span class="line">            SAVE_HOOK.isAlive();</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"no我死了"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码可以自己运行一下，还是有收获的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1.引用计数算法<br>早期判断对象是否存活大多都是以这种算法，这种算法判断很简单，简单来说就是给对象添加一个引用计数器，每当对象被引用一次就加1，引用失效时就减1。当为0的时候就判断对象不会再被引用。<br>优点:实现简单效率高，被广泛使用与如python何游戏脚本语言上。<br>缺点:难以解决循环引用的问题，就是假如两个对象互相引用已经不会再被其它其它引用，导致一直不会为0就无法进行回收。</p><p>2.可达性分析算法<br>目前主流的商用语言[如java、c#]采用的是可达性分析算法判断对象是否存活。这个算法有效解决了循环利用的弊端。<br>它的基本思路是通过一个称为“GC Roots”的对象为起始点，搜索所经过的路径称为引用链，当一个对象到GC Roots没有任何引用跟它连接则证明对象是不可用的。<br>要真正宣告对象死亡需经过两个过程。<br>1.可达性分析后没有发现引用链<br>2.查看对象是否有finalize方法，如果有重写且在方法内完成自救[比如再建立引用]，还是可以抢救一下，注意这边一个类的finalize只执行一次，这就会出现一样的代码第一次自救成功第二次失败的情况。[如果类重写finalize且还没调用过，会将这个对象放到一个叫做F-Queue的序列里，这边finalize不承诺一定会执行，这么做是因为如果里面死循环的话可能会时F-Queue队列处于等待，严重会导致内存崩溃，这是我们不希望看到的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;垃圾回收之引用计数算法和可达性分析算法&quot;&gt;&lt;a href=&quot;#垃圾回收之引用计数算法和可达性分析算法&quot; class=&quot;headerlink&quot; title=&quot;垃圾回收之引用计数算法和可达性分析算法&quot;&gt;&lt;/a&gt;垃圾回收之引用计数算法和可达性分析算法&lt;/h1&gt;&lt;h2 i</summary>
      
    
    
    
    <category term="java" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/java/"/>
    
    
    <category term="jvm" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>数据治理思考</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/22/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E6%80%9D%E8%80%83/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/22/%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%E6%80%9D%E8%80%83/</id>
    <published>2022-09-22T13:15:23.000Z</published>
    <updated>2022-10-14T08:10:57.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据治理"><a href="#数据治理" class="headerlink" title="数据治理"></a>数据治理</h2><p>数据治理通常包含比较琐碎的步骤，细节点很多，需要一个完善的体系来框柱这些细节，这样在后续执行数据治理给出方案的时候才能有一个完备的思路。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>在数据治理上，主要问题点如下：</p><ul><li><strong>治理认知差异大</strong><ul><li><strong>认知不一致，思路不统一</strong>：治理缺乏通用的体系指引，不同的治理人对于数据治理的认知深度、问题拆解的方式、治理的思路步骤、采取的方法及其效果追踪等方面，都存在较大的差异。</li><li><strong>重复治理、信息不通</strong>：治理不彻底、治理经验缺乏沉淀，同样的治理，不同的人反复实行。</li><li><strong>范围交叉、边界不清、效果难评估</strong>：不同的人针对不同的问题成立不同的专项进行治理，问题的底层逻辑有交叉。有的治理没做什么动作，反而收到了较好的结果，有的治理对于结果说不清。</li></ul></li><li><strong>治理方法不标准</strong><ul><li><strong>流程规范缺失</strong>：对于每个方向、每类问题的治理缺少理论指导，治理的方法、动作、流程、步骤依赖治理人的经验和判断。</li><li><strong>问题难度量追踪</strong>：治理的问题缺少衡量标准，更多靠人为来进行判断，治理效果缺少评估体系。</li><li><strong>解决方案难落地</strong>：解决方案存在于文档中，需要治理人查找理解，缺少工具支撑，成本较高。</li></ul></li><li><strong>治理效率低、效果差</strong><ul><li><strong>治理线上化程度低</strong>：治理依赖的资产信息、治理动作都分散于多个系统中，信息碎片化，执行效率低。</li><li><strong>过程无法标准化，结果无保障</strong>：治理过程需要治理人来“人为保障”，存在理解偏差和执行偏差。</li></ul></li><li><strong>数据管治缺乏体系化</strong><ul><li><strong>缺乏整体顶层治理方案设计</strong>：业务及数据中心对于数据治理的要求，需要治理更全面、更精细、更有效，需要治理的体系化，需要从宏观角度进行思考，层层拆解，需要从整体、从顶层来做方案设计。</li><li><strong>问题越来越复杂，单点难解决</strong>：过往更多的是从表象去解决问题，从表面来看衡量指标有改善，实际是“头痛医头、脚痛医脚”，并没有从根本上解决问题。或者多个问题具有共性，根本问题是一致的。比如查询资源紧张的根本，可能是分析主题模型建设不足或运营不够。</li><li><strong>不同问题的优先级无法确定</strong>：不同问题的优先级缺乏衡量标准和方法，主要靠人为判断。</li><li><strong>治理不符合MECE原则</strong>：每个治理方向由哪些问题组成，哪些最重要，哪些的ROI最高，哪些问题和治理动作可以合并，同一问题在数仓不同主题、不同分层的衡量标准和治理方法应该有哪些差异，都需要在体系化治理中进行考虑。</li></ul></li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>从上述背景中不难看出，我们面临着不同业务生命周期阶段对数据建设和治理不同的要求及挑战，同时过往更多的以被动治理、问题驱动的专项治理方式方法也比较落后，这直接导致技术团队很难满足业务方对于财务、业务支持等方面的要求。</p><p>只有构建出一套标准的业务数据管治体系，才能确保数据治理在现状评估、目标制定、流程规范建设、治理监控管理、能力建设、执行效率、效果评价等各环节有效落地。下面介绍一下我们在治理体系化层面的理解和思考。</p><p>针对数据管理和治理，我们期望搭建一套集管理体系、方法体系、评价体系、标准体系、工具体系等核心能力的组合，持续服务于数据管治实施。可以类比一般的电商公司，如果需要运转并服务好顾客，它首先必须搭建起来一套销售体系、产品体系、供给体系、物流体系、人力体系等等，只有这样才可以相互配合，实现服务好用户这一大目标。</p><p><img src="/images/7b271ac28689985760164599b5601e2738302.jpg" alt=""></p><blockquote><p>还是产品化思维入手，先弄清楚治理的目标，然后围绕治理的目标来涉及这套产品，需要完整的闭环。</p></blockquote><h3 id="如何做呢"><a href="#如何做呢" class="headerlink" title="如何做呢"></a>如何做呢</h3><ul><li><strong>方式方法上</strong>：先做<strong>顶层治理框架</strong>设计，从团队整体视角定义和规划好治理的<strong>范围、人员、职责、目标、方法、工具等必须</strong>部分，再进行落地。更关注整体策略的<strong>普适性及有效性</strong>，而非深陷某个具体问题解决方案开始治理。</li><li><strong>技术手段上</strong>：以完善的技术研发规范为基础，以<strong>元数据及指标体系</strong>为核心，对业务数仓和数据应用进行<strong>全面评价和监控</strong>，同时配套治理系统工具，帮助治理同学落地治理策略和解决数据开发同学治理效率低问题。</li><li><strong>运营策略上</strong>：通过对待治理问题进行影响范围、收益情况进行评估，确定待治理问题的重要度，从<strong>管理者视角以及问题责任人视角</strong>2个途径推动不同重要程度的治理问题解决。</li></ul><blockquote><p>元数据和指标体系为核心，所以这两者会是重中之重。</p></blockquote><h3 id="过程如何推进"><a href="#过程如何推进" class="headerlink" title="过程如何推进"></a>过程如何推进</h3><p>以团队数据治理目标为核心导向，设计实现目标需要的<strong>相关能力组合</strong>，并根据组织要求，实施过程的问题反馈，持续不断地迭代完善，最终实现数据治理的愿景。</p><ul><li><strong>管理层</strong>：立法，制定相关的<strong>组织保障流程规范、职责设计、奖惩措施</strong>，指导和保障数据治理顺利进行，这是数据治理能够成功启动运转的关键因素。</li><li><strong>标准层</strong>：设标准，制定各类<strong>研发标准规范、解决方案标准SOP等</strong>数据治理过程中需要的各类技术规范和解决方案，这是所有技术问题正确与否的重要依据，也是治理中事前解决方案必不可少的一部分。完善的标准规范和良好的落地效果，可很好地降低数据故障问题的发生量。</li><li><strong>能力层</strong>：完善能力，主要是基于<strong>元数据的问题度量的数字化能力</strong>，以及问题工具化检测和解决的系统化能力。数字化和系统化能力是数据治理实施的科学性、实施的质量及效率的重要保障。</li><li><strong>执行层</strong>：设定动作，结合要达成的具体目标，对各治理域问题，按照<strong>事前约束、事中监控、事后治理</strong>的思路进行解决。目标的达成，需要拆分到7大治理域相关的具体问题中去落地。因此，<strong>一个治理目标的达成，很依赖治理域对问题描述的全面性及深度</strong>。</li><li><strong>评价层</strong>：给出评价，基于指标的问题监控，健康度评价体系，专项评估报告，评价治理收益及效果，这是实施治理推进过程监控，结果检验的重要抓手。</li><li><strong>愿景</strong>：长期治理目标，指导数据管治有方向地不断朝着最终目标前进。</li></ul><p><img src="/images/480178c4a8ea94a3dca70c9a9b2c1d9859056.png" alt=""></p><p><img src="/images/fd155377cd454577f0c6f5293df217bf639336.png" alt=""></p><blockquote><p>框架是认知的规范</p></blockquote><h3 id="落地实施"><a href="#落地实施" class="headerlink" title="落地实施"></a>落地实施</h3><p><img src="/images/eb4d7dd76596239872bd691d55f73a46182162.png" alt=""></p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h3><p><img src="/images/317d109ca5925df757f5a3b0a98b859d38730.jpg" alt=""></p><ul><li><strong>数据治理管理规范</strong>：明确数据治理组织职责以及人员构成，确定数据治理实施流程及治理问题运维流程，以保障数据治理过程顺利进行。</li><li><strong>数据研发规范</strong>：明确数据开发各个环节需要遵守的规范要求，从问题产生的源头，通过建设完善的研发规范，指导研发工作按标准进行，一定程度上可减少问题发生。</li><li><strong>数据标准化治理 SOP</strong>：明确各个治理问题治理动作，确保治理动作是标准且可实施。</li><li><strong>数据健康度评估规范</strong>：明确治理效果的评价标准，对数据体系做到长期，稳定及指标化的衡量。</li></ul><p>通过数据治理标准化建设，我们解决了团队在数据治理规范方面若干问题，取得了明显效果：</p><ul><li>实现了数据开发、数据治理的标准化，解决了团队内各小组之间在开发、管理、运维方面流程方法标准不一致的问题。</li><li>通过测试工具对标准化测试规范进行落地，在事前阻塞问题发生，提升数据质量，减少故障发生。</li><li>通过 SOP 自动化工具，有效保障治理过程的标准化，解决了治理效果差的问题。</li></ul><p>同时，我们在实际建设的过程中，也总结了一些标准化的建设经验：</p><ul><li>标准规范如何落地，需成为标准流程规范建设的一部分，最好有交付物。</li><li>标准规范的制定，除常规内容外，需要综合考虑组织目标、组织特点、已有工具、历史情况、用户反馈等因素，否则会给人 “不接地气” 的感觉。</li><li>标准规范的制定要优先考虑利用和适配已有工具能力，借助工具落地，而非让工具适配流程规范。</li></ul><blockquote><p>所有规范要都切实可行地体现在工具上，这样才能是有保障的规范，否则就不了了之了。</p></blockquote><h3 id="数字化"><a href="#数字化" class="headerlink" title="数字化"></a>数字化</h3><h4 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h4><p>建设思路：通过对数据生命周期各环节进行类比业务数仓建设中抽象和描述业务对象方式，进行元数据对象的抽象和描述，并建设成元数据数仓和治理指标体系，应用在数据管治场景。</p><p>框架主要包含元数据仓库、指标体系、数据资产等级以及基于元数仓基础上建立的各个数据应用，利用元数据驱动数据治理及日常团队管理，避免过多依赖经验解决问题，更好地服务业务。下边几个章节将分别介绍数字化框架最核心的数据内容：元数据仓库、指标体系、数据资产等级。</p><p><img src="/images/debc32c8b50dd4b63ffca2fddd0291fa121954.jpg" alt=""></p><p>通过数据业务化思路，我们抽象业务域、管理域、技术域等 3 大主题域来描述元数仓对象，并对每个主题域进行细分，划分多个主题：</p><ul><li><strong>业务元数据</strong>：基于具体业务逻辑元数据，常见业务元数据包括业务定义、业务术语、业务规则、业务指标等。</li><li><strong>技术元数据</strong>：描述了与数据仓库开发、管理和维护相关数据，包括数据源信息、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等，主要为开发和管理数据仓库的工程师使用。</li><li><strong>管理元数据</strong>：描述管理领域相关概念、关系和规则的数据，主要包括管理流程、人员组织、角色职责等信息。</li></ul><p>在元数仓分层上，我们采用最常见的四层架构分层方式，分别是<strong>贴源层、明细层、汇总层、应用层和维度信息</strong>。区别于业务数仓分层设计方式，从明细层就按维度建模思路组织数据，<strong>避免过度设计，只需要做好主题划分和解耦</strong>。在汇总层从分析习惯出发耦合数据，提升易用性。应用层按需创建所需接口支撑应用。</p><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>指标体系的建设目标是监控团队工作状态和变化趋势，需要能够覆盖到工作中的各个方面。因此，在指标体系的建设上，我们通过不同视角对指标体系进行分类，做到不重不漏全覆盖，让指标适用于不同使用场景：</p><ul><li><strong>生命周期视角</strong>：从数据本身出发，衡量数据从生产到销毁的各个过程，包括定义、接入、处理、存储、使用、销毁等等。</li><li><strong>团队管理目标视角</strong>：根据团队管理核心要达成的目标分类，包括质量、效率、成本、安全、易用性、价值等等。</li><li><strong>问题对象视角</strong>：根据治理问题核心关注的对象分类，包括安全、资源、服务、架构、效率、价值、质量等等。</li></ul><p>注意：</p><ul><li>指标体系既要解决管理者对日常工作无抓手的问题，也要成为具体问题处理人员的治理抓手，兼顾管理者和开发者。</li><li>指标体系是展示偏整体层面的内容，还需通过指标解决实际问题，形成指标体系和数据治理工具闭环，实现发现问题、治理问题、衡量结果持续循环。</li><li>优先确定团队总体发展目标，从目标拆分设定指标，指标尽量覆盖不同业务线不同发展阶段。</li><li>业务需要明确自己所处阶段，针对不同阶段，制定考核目标，衡量阀值，既统一了衡量标准，又中和了大家考核标准。</li><li>指标需注意分层建设，避免 “胡子眉毛一把抓”，便于适配目前的组织结构，也便于划分责任与定位。</li><li>基础指标体系建设完成后，可作为平时管理和工作的抓手，作为项目发起的依据，作为项目结果评估的手段。</li></ul><h4 id="资产"><a href="#资产" class="headerlink" title="资产"></a>资产</h4><p><img src="/images/f7248239d2d81c7fc176c9cbf483857e22070.jpg" alt=""></p><p><img src="/images/fdc9542bc28d01d57a1a84db820fef1737889.jpg" alt=""></p><p><img src="/images/e0fdba86828b36d737a69863d515ce33175184.jpg" alt=""></p><h3 id="系统化"><a href="#系统化" class="headerlink" title="系统化"></a>系统化</h3><p><img src="/images/ec3430a249ccf20b62b6b48094786444189849.jpg" alt=""></p><p><img src="/images/2e64b89bcce15b6e558aa0cfc5eda6e388656.jpg" alt=""></p><p><img src="/imagse/e525bb237841409574343d42ab5d3e56110990.jpg" alt=""></p><p><img src="/images/41417206dbc1604e5c523b019f8b9885188444.jpg" alt=""></p><p><img src="/images/f41f3e9d8eae6b5350d24c5436e07a1c53643.jpg" alt=""></p><h3 id="实施流程"><a href="#实施流程" class="headerlink" title="实施流程"></a>实施流程</h3><ul><li>STEP 1：发现问题和制定目标，发现问题要从业务数据开发团队的视角出发，围绕服务好业务、遵守数据研发规范、收集好用户反馈，尽可能全地发现和收集相关需要解决的问题。同时，制定的目标要具备可实现性。</li><li>STEP 2：针对问题进行拆解，设计可衡量的指标，并通过元数据的采集建设进行实现，用做对目标的进一步量化，并作为实施过程监控及治理抓手。</li><li>STEP 3：对衡量出来的具体问题，制定相关的解决 SOP，并且检查相应的研发标准规范是否完善，通过问题发生的事前、事中、事后几个阶段，建设或完善相应的工具化解决问题的能力。</li><li>STEP 4：推广运营，以拿结果为核心目标，针对不同角色运用不同策略，重点关注问题解决过程是否会与用户利益发生冲突，控制好节奏，根据问题的重要程度有规划地进行解决。</li><li>STEP 5：总结沉淀方法论，迭代认知，持续探索问题的最优解，优化治理方案和能力。</li></ul><p><img src="/images/1f7ad993a9c3f1ba622fc53e51fbe33e45027.jpg" alt=""></p><p><strong>注：以上内容来自美团技术博客的文章，下方有标注。我没有这方面落地的经验，但是很感兴趣，所以看到了这篇含金量很高的文章，选择我认为重要的部分进行了摘抄，如有侵权，请联系我删除，谢谢。</strong></p><p>参考：</p><p><a href="https://tech.meituan.com/2022/05/12/business-data-governance.html" target="_blank" rel="noopener">业务数据治理体系化思考与实践</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;数据治理&quot;&gt;&lt;a href=&quot;#数据治理&quot; class=&quot;headerlink&quot; title=&quot;数据治理&quot;&gt;&lt;/a&gt;数据治理&lt;/h2&gt;&lt;p&gt;数据治理通常包含比较琐碎的步骤，细节点很多，需要一个完善的体系来框柱这些细节，这样在后续执行数据治理给出方案的时候才能有一个</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s架构初识</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/22/k8s%E6%9E%B6%E6%9E%84%E5%88%9D%E8%AF%86/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/22/k8s%E6%9E%B6%E6%9E%84%E5%88%9D%E8%AF%86/</id>
    <published>2022-09-22T12:37:33.000Z</published>
    <updated>2022-10-12T08:36:17.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k8s架构初识"><a href="#k8s架构初识" class="headerlink" title="k8s架构初识"></a>k8s架构初识</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Kubernetes，又称为 k8s（首字母为 k、首字母与尾字母之间有 8 个字符、尾字母为 s，所以简称 k8s）或者简称为 “kube” ，是一种可自动实施 Linux 容器操作的开源平台。它可以帮助用户省去应用容器化过程的许多手动部署和扩展操作。也就是说，<strong>您可以将运行 Linux 容器的多组主机聚集在一起，由 Kubernetes 帮助您轻松高效地管理这些集群</strong>。而且，这些集群<strong>可跨公共云、私有云或混合云部署主机</strong>。因此，对于要求快速扩展的云原生应用而言（例如借助 Apache Kafka 进行的实时数据流处理），Kubernetes 是理想的托管平台。</p><p>Kubernetes 最初由 Google 的工程师开发和设计。Google 是最早研发 Linux 容器技术的企业之一（组建了cgroups），曾公开分享介绍 Google 如何将一切都运行于容器之中（这是 Google 云服务背后的技术）。Google 每周会启用超过 20 亿个容器——全都由内部平台 Borg 支撑。Borg 是 Kubernetes 的前身，多年来开发 Borg 的经验教训成了影响 Kubernetes 中许多技术的主要因素。</p><h2 id="为什么需要-Kubernetes？"><a href="#为什么需要-Kubernetes？" class="headerlink" title="为什么需要 Kubernetes？"></a>为什么需要 Kubernetes？</h2><p>真正的生产型应用会涉及多个容器。这些容器必须跨多个服务器主机进行部署。容器安全性需要多层部署，因此可能会比较复杂。但 Kubernetes 有助于解决这一问题。Kubernetes 可以提供所需的编排和管理功能，以便您针对这些工作负载大规模部署容器。借助 Kubernetes 编排功能，您可以构建跨多个容器的应用服务、跨集群调度、扩展这些容器，并长期持续管理这些容器的健康状况。有了 Kubernetes，您便可切实采取一些措施来提高 IT 安全性。</p><p>Kubernetes 还需要与联网、存储、安全性、遥测和其他服务整合，以提供全面的容器基础架构。</p><blockquote><p>基本上就是为了更好整合这些机器，然后在易用性、安全性上进行大量的整合改进，这样就产生了K8S的需求背景。</p></blockquote><p><img src="https://www.redhat.com/cms/managed-files/styles/wysiwyg_full_width/s3/kubernetes-diagram-902x416.png?itok=8uBj2yXl" alt=""></p><p>当然，这取决于您如何在您的环境中使用容器。Linux 容器中的基本应用将它们视作高效、快速的虚拟机。<strong>一旦把它部署到生产环境或扩展为多个应用，您显然需要许多托管在相同位置的容器来协同提供各种服务</strong>。随着这些容器的累积，您运行环境中容器的数量会急剧增加，复杂度也随之增长。</p><p>Kubernetes 通过将容器分类组成”容器集” （pod） ，解决了容器增殖带来的许多常见问题容器集为分组容器增加了一个抽象层，可帮助您调用工作负载，并为这些容器提供所需的联网和存储等服务。Kubernetes 的其它部分可帮助您在这些容器集之间达成负载平衡，同时确保运行正确数量的容器，充分支持您的工作负载。</p><p>如果能正确实施 Kubernetes，再辅以其它开源项目（例如 Atomic 注册表、Open vSwitch、heapster、OAuth 以及 SELinux），您就能够轻松编排容器基础架构的各个部分。</p><h2 id="Kubernetes-设计原则"><a href="#Kubernetes-设计原则" class="headerlink" title="Kubernetes 设计原则"></a>Kubernetes 设计原则</h2><ul><li>安全。它应遵循最新的安全最佳实践。</li><li>易于使用。它应能通过一些简单的命令进行操作。 </li><li>可扩展。不应偏向于某一个提供商，而是能通过配置文件进行自定义。</li></ul><h2 id="Kubernetes-有哪些应用？"><a href="#Kubernetes-有哪些应用？" class="headerlink" title="Kubernetes 有哪些应用？"></a>Kubernetes 有哪些应用？</h2><p>在您生产环境中（尤其是当您要面向云优化应用开发时）使用 Kubernetes 的主要优势在于，<strong>它提供了一个便捷有效的平台，让您可以在物理机和虚拟机集群上调度和运行容器</strong>。更广泛一点说，<strong>它可以帮助您在生产环境中，完全实施并依托基于容器的基础架构运营</strong>。由于 Kubernetes 的实质在于实现操作任务自动化，所以您可以将其它应用平台或管理系统分配给您的许多相同任务交给容器来执行。</p><p>利用 Kubernetes，您能够达成以下目标：</p><ul><li>跨多台主机进行容器编排。</li><li>更加充分地利用硬件，最大程度获取运行企业应用所需的资源。</li><li>有效管控应用部署和更新，并实现自动化操作。</li><li>挂载和增加存储，用于<strong>运行有状态的应用</strong>。</li><li>快速、按需扩展容器化应用及其资源。</li><li>对服务进行声明式管理，保证所部署的应用始终按照部署的方式运行。</li><li>利用自动布局、自动重启、自动复制以及自动扩展功能，对应用实施状况检查和自我修复。</li><li>但是，Kubernetes 需要依赖其它项目来全面提供这些经过编排的服务。因此，借助其它开源项目可以帮助您将 Kubernetes 的全部功用发挥出来。这些功能包括：</li><li>注册表，通过 Atomic 注册表或 Docker 注册表等项目实现。</li><li>联网，通过 OpenvSwitch 和智能边缘路由等项目实现。</li><li>遥测，通过 heapster、kibana、hawkular 和 elastic 等项目实现。</li><li>安全性，通过 LDAP、SELinux、RBAC 和 OAUTH 等项目以及多租户层来实现。</li><li>自动化，参照 Ansible 手册进行安装和集群生命周期管理。</li><li>服务，可通过自带预建版常用应用模式的丰富内容目录来提供。</li></ul><h2 id="Kubernetes-相关术语"><a href="#Kubernetes-相关术语" class="headerlink" title="Kubernetes 相关术语"></a>Kubernetes 相关术语</h2><p>Kubernetes 也会采用一些专用的词汇，这可能会对初学者理解和掌握这项技术造成一定的障碍。为了帮助您了解 Kubernetes，我们在下面来解释一些常用术语。</p><ul><li>主机（Master）： 用于控制 Kubernetes 节点的计算机。所有任务分配都来自于此。</li><li>节点（Node）：负责执行请求和所分配任务的计算机。由 Kubernetes 主机负责对节点进行控制。</li><li>容器集（Pod）：被部署在单个节点上的，且包含一个或多个容器的容器组。同一容器集中的所有容器共享同一个 IP 地址、IPC、主机名称及其它资源。容器集会将网络和存储从底层容器中抽象出来。这样，您就能更加轻松地在集群中移动容器。</li><li>复制控制器（Replication controller）：用于控制应在集群某处运行的完全相同的容器集副本数量。</li><li>服务（Service）：将工作内容与容器集分离。Kubernetes 服务代理会自动将服务请求分发到正确的容器集——无论这个容器集会移到集群中的哪个位置，甚至可以被替换掉。</li><li>Kubelet：运行在节点上的服务，可读取容器清单（container manifest），确保指定的容器启动并运行。</li><li>kubectl： Kubernetes 的命令行配置工具。</li></ul><p>我们把一个有效的 Kubernetes 部署称为集群。您可以将 Kubernetes 集群可视化为两个部分：控制平面与计算设备（或称为节点）。每个节点都是其自己的 Linux® 环境，并且可以是物理机或虚拟机。每个节点都运行由若干容器组成的容器集。</p><p><img src="https://www.redhat.com/cms/managed-files/kubernetes_diagram-v3-770x717_0.svg" alt=""></p><h3 id="Kubernetes-控制平面中会发生什么？"><a href="#Kubernetes-控制平面中会发生什么？" class="headerlink" title="Kubernetes 控制平面中会发生什么？"></a>Kubernetes 控制平面中会发生什么？</h3><h4 id="控制平面"><a href="#控制平面" class="headerlink" title="控制平面"></a>控制平面</h4><p>在这里，我们可以找到用于控制集群的 Kubernetes 组件以及一些有关集群状态和配置的数据。这些核心 Kubernetes 组件负责处理重要的工作，以确保容器以足够的数量和所需的资源运行。<br>控制平面会一直与您的计算机保持联系。集群已被配置为以特定的方式运行，而控制平面要做的就是确保万无一失。</p><h4 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h4><p>如果需要与您的 Kubernetes 集群进行交互，就要通过 API。Kubernetes API 是 Kubernetes 控制平面的前端，用于处理内部和外部请求。API 服务器会确定请求是否有效，如果有效，则对其进行处理。您可以通过 REST 调用、kubectl 命令行界面或其他命令行工具（例如 kubeadm）来访问 API。</p><h4 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h4><p>您的集群是否状况良好？如果需要新的容器，要将它们放在哪里？这些是 Kubernetes 调度程序所要关注的问题。<br>调度程序会考虑容器集的资源需求（例如 CPU 或内存）以及集群的运行状况。随后，它会将容器集安排到适当的计算节点。</p><h4 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h4><p>控制器负责实际运行集群，而 Kubernetes 控制器管理器则是将多个控制器功能合而为一。控制器用于查询调度程序，并确保有正确数量的容器集在运行。如果有容器集停止运行，另一个控制器会发现并做出响应。控制器会将服务连接至容器集，以便让请求前往正确的端点。还有一些控制器用于创建帐户和 API 访问令牌。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>配置数据以及有关集群状态的信息位于 etcd（一个键值存储数据库）中。etcd 采用分布式、容错设计，被视为集群的最终事实来源。</p><h3 id="Kubernetes-节点中会发生什么？"><a href="#Kubernetes-节点中会发生什么？" class="headerlink" title="Kubernetes 节点中会发生什么？"></a>Kubernetes 节点中会发生什么？</h3><h4 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h4><p>Kubernetes 集群中至少需要一个计算节点，但通常会有多个计算节点。容器集经过调度和编排后，就会在节点上运行。如果需要扩展集群的容量，那就要添加更多的节点。</p><h4 id="容器集"><a href="#容器集" class="headerlink" title="容器集"></a>容器集</h4><p>容器集是 Kubernetes 对象模型中最小、最简单的单元。它代表了应用的单个实例。每个容器集都由一个容器（或一系列紧密耦合的容器）以及若干控制容器运行方式的选件组成。容器集可以连接至持久存储，以运行有状态应用。</p><h4 id="容器运行时引擎"><a href="#容器运行时引擎" class="headerlink" title="容器运行时引擎"></a>容器运行时引擎</h4><p>为了运行容器，每个计算节点都有一个容器运行时引擎。比如 Docker，但 Kubernetes 也支持其他符合开源容器运动（OCI）标准的运行时，例如 rkt 和 CRI-O。</p><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>每个计算节点中都包含一个 kubelet，这是一个与控制平面通信的微型应用。kublet 可确保容器在容器集内运行。当控制平面需要在节点中执行某个操作时，kubelet 就会执行该操作。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>每个计算节点中还包含 kube-proxy，这是一个用于优化 Kubernetes 网络服务的网络代理。kube-proxy 负责处理集群内部或外部的网络通信——靠操作系统的数据包过滤层，或者自行转发流量。</p><h2 id="Kubernetes-集群还需要些什么？"><a href="#Kubernetes-集群还需要些什么？" class="headerlink" title="Kubernetes 集群还需要些什么？"></a>Kubernetes 集群还需要些什么？</h2><h3 id="持久存储"><a href="#持久存储" class="headerlink" title="持久存储"></a>持久存储</h3><p>除了管理运行应用的容器外，Kubernetes 还可以管理附加在集群上的应用数据。Kubernetes 允许用户请求存储资源，而无需了解底层存储基础架构的详细信息。持久卷是集群（而非容器集）所特有的，因此其寿命可以超过容器集。</p><h3 id="容器镜像仓库"><a href="#容器镜像仓库" class="headerlink" title="容器镜像仓库"></a>容器镜像仓库</h3><p>Kubernetes 所依赖的容器镜像存储于容器镜像仓库中。这个镜像仓库可以由您自己配置的，也可以由第三方提供。</p><h3 id="底层基础架构"><a href="#底层基础架构" class="headerlink" title="底层基础架构"></a>底层基础架构</h3><p>您可以自己决定具体在哪里运行 Kubernetes。答案可以是裸机服务器、虚拟机、公共云提供商、私有云和混合云环境。Kubernetes 的一大优势就是它可以在许多不同类型的基础架构上运行。</p><p>Kubernetes 提供了编排复杂的大型容器化应用所需的工具，但也为您留出了许多决策空间。您要选择操作系统、容器运行时、持续集成/持续交付（CI/CD）工具、应用服务、存储以及其他大多数组件。另外，还有管理角色、访问控制、多租户和安全默认设置等工作需要您来完成。不仅如此，您也可以选择自行运行 Kubernetes，或是与能够提供受支持版本的供应商展开合作。</p><p>这种选择的自由度正是 Kubernetes 灵活性的一大表现。尽管实施起来可能比较复杂，但 Kubernetes 赋予了您强大的功能，可以按照自己的条件运行容器化应用，并以敏捷的方式对组织的变化做出回应。</p><blockquote><p>k8s的很多点都有很好的思想，值得细细品味。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;k8s架构初识&quot;&gt;&lt;a href=&quot;#k8s架构初识&quot; class=&quot;headerlink&quot; title=&quot;k8s架构初识&quot;&gt;&lt;/a&gt;k8s架构初识&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概</summary>
      
    
    
    
    <category term="云原生" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
    
    <category term="云原生" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
  </entry>
  
  <entry>
    <title>hudi目前存在的一些问题记录</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/hudi%E7%9B%AE%E5%89%8D%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/hudi%E7%9B%AE%E5%89%8D%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</id>
    <published>2022-09-21T07:41:39.000Z</published>
    <updated>2022-10-09T11:53:02.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hudi目前存在的问题"><a href="#hudi目前存在的问题" class="headerlink" title="hudi目前存在的问题"></a>hudi目前存在的问题</h1><p>最近在深入测试hudi，目前测试的版本是0.10.0<br>这里是一个长期跟进的帖子，自己会不断补充进来一些信息：当前时间：<strong>2022-09-21</strong></p><h2 id="hudi不支持多层次嵌套"><a href="#hudi不支持多层次嵌套" class="headerlink" title="hudi不支持多层次嵌套"></a>hudi不支持多层次嵌套</h2><p>这个问题是大部分引擎都不支持的，因为多层次嵌套引起来的问题会比较多，不管是写入还是读取，对于元数据以及相关操作的挑战都会更上一个台阶。<br>当前hudi是对于基础数据类型的支持是足够全面的，复杂数据类型，已经支持了map、array，但是也只是支持嵌套单层次，也就是里边的元素是基础数据类型，不能array里包含的是map，map再去包含map这种数据类型，就会被无法解析。<br>对于map和array的支持也是在0.11.0的版本开始支持的，读取流程跟spark读取parquet的代码逻辑很像。<br>但是spark对于读取parquet实际是完全支持的，也就是多层次嵌套结构spark是可以直接支持的，目前在hudi这一块还需要进行同等类型的改进。parquet和avro对于这种类型的元数据的扩展都是足够丰富的。这块的代码其实可以参照spark读取parquet的实现进行改进的。</p><blockquote><p>改进方式：参考spark读取parquet的代码实现。avro都是自带元数据的，这部分直接用avro的实现就可以了。</p></blockquote><h2 id="hudi的ddl"><a href="#hudi的ddl" class="headerlink" title="hudi的ddl"></a>hudi的ddl</h2><p>在0.11.0版本，hudi支持了schema evolution，也就是常见的alter语句的支持。<br>对比一下ddl语句在mysql里的实现，也是一个很耗时的操作，主要的实现方式也是通过异步创建全量+增量同步的方式来避免停表，减小影响。但是在有一些场景，ddl还是要务必停表的，因为会造成无法修补的脏数据或者造成很难解决的数据异常。<br>在oltp的ddl基本都是覆盖的很全的，即使执行成本很高，也都是可以执行的。但是在hudi这里，目前的ddl是支持部分的。<br>目前主要包括：</p><ul><li>增加列</li><li>修改列的位置</li><li>修改列的comment</li><li>修改列的数据类型：支持数据变宽，不支持反向。</li><li>修改列名</li><li>删除列</li><li>修改表属性</li><li>修改表名称<br>这里还涉及一些ddl，如truncate、复杂数据类型的列字段修改、还有就会针对历史数据的改动、针对不同引擎的兼容。<br>上边四点是目前hudi在这个方向上使用的主要问题。比如历史数据没有覆盖，那么读取的时候就会报错。再比如，针对flink和spark，对于ddl的支持的实现是不一样的。<blockquote><p>我认为解决方案就是hudi metastore的使用，在每次加载的时候，都要去加载这个，但是线上的ddl语句要直接被应用到hudi自己的metastore。在查询的时候，可以直接根据这个做转化，这里涉及在后续写入以及历史数据的读取的覆盖。</p></blockquote></li></ul><h2 id="下推"><a href="#下推" class="headerlink" title="下推"></a>下推</h2><p>针对cow表，可以实现很好的下推，因为都是基于parquet存储的，但是对于mor表，就暴露出一个问题了，他需要先进行数据的merge，才能算是可用的数据，才能去被过滤，否则直接拿base文件的话，会存在数据不准确的问题。<br>针对mor表不能下推的情况，会造成查询效率低下。而我认为终极一点的解决方案就是靠索引。</p><blockquote><p>针对0.11版本推出来的可扩展索引其实是个很好的解决方式，不过在考虑用什么索引以及如何维护这个索引，当数据量比较大的时候，是否会存在更新的延迟问题。</p></blockquote><h2 id="flink写入hudi存在小文件问题"><a href="#flink写入hudi存在小文件问题" class="headerlink" title="flink写入hudi存在小文件问题"></a>flink写入hudi存在小文件问题</h2><p>当hudi表什么都没有的时候，直接用flink对mor类型的hudi表进行写入，flink只会写入log文件，但是log文件不进行压缩，hudi在fileId的形成的时候，也就是多少数据使用一个fileId这件事儿，就会存在偏差。当进行compact的时候，会发现base文件只有20多M，这回形成大量的小文件。</p><blockquote><p>形成fileId的预估算法是需要改进的。需要按照base文件来预估。base文件通常是可以压缩的。</p></blockquote><h2 id="presto无法查询mor表的问题"><a href="#presto无法查询mor表的问题" class="headerlink" title="presto无法查询mor表的问题"></a>presto无法查询mor表的问题</h2><p>查询mor表，需要用base数据和log数据做合并，这部分运行会对资源消耗比较大，因为无法下推的原因，里边包含的很多merge其实在最终也是无效的。</p><blockquote><p>索引会是比较考虑的解决方案，毕竟presto的资源是在线固定的。第二种方案就会用读优化的模式，但是数据的实时性会有影响，依赖于compact的粒度。</p></blockquote><h2 id="元数据不一致"><a href="#元数据不一致" class="headerlink" title="元数据不一致"></a>元数据不一致</h2><p>flink和spark的元数据不一致，这一部分，对于不同的引擎，没有兼容。如果有夸两个引擎的数据生产，这个过程需要注意。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;hudi目前存在的问题&quot;&gt;&lt;a href=&quot;#hudi目前存在的问题&quot; class=&quot;headerlink&quot; title=&quot;hudi目前存在的问题&quot;&gt;&lt;/a&gt;hudi目前存在的问题&lt;/h1&gt;&lt;p&gt;最近在深入测试hudi，目前测试的版本是0.10.0&lt;br&gt;这里是一</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="hudi" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/hudi/"/>
    
  </entry>
  
  <entry>
    <title>管理者一定会碰到的那些事儿</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/%E7%AE%A1%E7%90%86%E8%80%85%E4%B8%80%E5%AE%9A%E4%BC%9A%E7%A2%B0%E5%88%B0%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/%E7%AE%A1%E7%90%86%E8%80%85%E4%B8%80%E5%AE%9A%E4%BC%9A%E7%A2%B0%E5%88%B0%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</id>
    <published>2022-09-21T02:57:05.000Z</published>
    <updated>2022-09-21T07:29:19.066Z</updated>
    
    <content type="html"><![CDATA[<h1 id="管理者一定会碰到的那些事儿"><a href="#管理者一定会碰到的那些事儿" class="headerlink" title="管理者一定会碰到的那些事儿"></a>管理者一定会碰到的那些事儿</h1><h2 id="不符合预期，是给机会，还是fire-fast"><a href="#不符合预期，是给机会，还是fire-fast" class="headerlink" title="不符合预期，是给机会，还是fire fast"></a>不符合预期，是给机会，还是fire fast</h2><p>针对不符合预期的员工还是要先找到原因，他到底是能力不到还是态度不到还是因为存在实际的客观因素。<br>要让员工把实际的困难反馈出来，这个很重要。反馈到了，才能知道问题在哪儿。<br>总结：<br>不符合预期的原因：</p><ul><li>能力确实没达到。这个时候需要给与机会，并给与帮助。</li><li>是否放在正确的岗位。在技术的什么时期对于技术的认知是不一样的，是需要逐层提升的。初期就不能放在太难的位置上。</li><li>帮助员工解决问题，是leader最重要的职责。</li><li>至少要给一次机会。<h2 id="员工不给力，要不要手把手指导"><a href="#员工不给力，要不要手把手指导" class="headerlink" title="员工不给力，要不要手把手指导"></a>员工不给力，要不要手把手指导</h2>员工不给力还是要确认都原因。在不同的阶段要给予不同的引导方式。初期阶段，还是要去手把手带，从代码细节到工程实现，从单个模块到整体架构，从问题排查到工具建立，要逐步完善思路。<br>总结：</li><li>细心、耐心、思路清晰，手把手指导</li><li>把员工变成牛逼的人，这才是领导该做的。</li><li>不抢工，不邀功，出现问题，作为领导要先顶上去，给员工安全感也是很有必要的。<h2 id="明明为了他好，沟通为何还是会谈崩"><a href="#明明为了他好，沟通为何还是会谈崩" class="headerlink" title="明明为了他好，沟通为何还是会谈崩"></a>明明为了他好，沟通为何还是会谈崩</h2>这个点上主要是沟通。沟通的方式其实是思维的转变。</li><li>不能用反问句。</li><li>以理服人的方式来他进行沟通。</li><li>要进行引导式的讨论。</li><li>讨论是技术进步最好的方式之一，即使是跟技术往往不如你的人那里，也是值得讨论的。<br>总结：</li><li>引导员工表达自己的意见</li><li>强势的反问句不可取</li><li>经验未必对，任何脱离业务的架构设计，都是耍流氓<h2 id="能把一切安排好，但是为什么会这么累"><a href="#能把一切安排好，但是为什么会这么累" class="headerlink" title="能把一切安排好，但是为什么会这么累"></a>能把一切安排好，但是为什么会这么累</h2>当支撑一个10人以下的团队的时候，可以关注到每个人的细节，也就可以安排的很细致。但是当团队很大的时候，就无法关注到对应的细节了。<br>所以，我们应该把自己的思维抽象成方法论，然后自己的思维地图去传播出去，让大家都得到case。<br>把员工推出去，让他们更靠前，这样可以让他们更快成长起来。比如一些技术演讲、一些方案展示等，可以让他们靠前去展示。出问题的时候，自己要去跟进和揽责，但是不是盲目的。<br>总结：</li><li>事必躬亲，不是最好的方式。而是应该授人以渔，让自己成为导师或者教练的角色。</li><li>不拿下属的成果去邀功，不推下属去背锅。而是应该把下属推向前台，得到更多的展示机会和影响力，从而可以快速成长。<h2 id="boss需求，如何应对"><a href="#boss需求，如何应对" class="headerlink" title="boss需求，如何应对"></a>boss需求，如何应对</h2>根据项目优先级以及如果产生新的项目插入之后对其他高优项目的影响，以此来作为对boss的反馈。要把项目变更的影响面反映到位，从而能够让boss来衡量变更带来的影响到底有多大。<br>总结：</li><li>优先级机制：公司战略为主导方向。</li><li>有优先级调整和变更的机制：优先级调整要与受影响项目的相关方都达成共识，让机制和流程都是在规范之内一直运行着。</li><li>“运动式”加班不可取，一定有，但是要尽量少，精力施展在刀刃上。</li><li>leader要专注于提升团队效率，平台化、工具、自动化，都是比较有力的手段。<h2 id="明明团队很优秀，但是投票结果不如愿"><a href="#明明团队很优秀，但是投票结果不如愿" class="headerlink" title="明明团队很优秀，但是投票结果不如愿"></a>明明团队很优秀，但是投票结果不如愿</h2>不能把投票结果看成是对自己的一个完全的评价，因为投票本身也不一定是完全公正的。因为投票本身可能存在私心。<br>自己要有一个激励机制，最好的激励方式就是自己激励自己。不能荒废自己的时间，也不能让自己的时间沉浸在别人的评价里。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;管理者一定会碰到的那些事儿&quot;&gt;&lt;a href=&quot;#管理者一定会碰到的那些事儿&quot; class=&quot;headerlink&quot; title=&quot;管理者一定会碰到的那些事儿&quot;&gt;&lt;/a&gt;管理者一定会碰到的那些事儿&lt;/h1&gt;&lt;h2 id=&quot;不符合预期，是给机会，还是fire-fas</summary>
      
    
    
    
    <category term="管理" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E7%AE%A1%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>sql-行列转换</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/sql-%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/21/sql-%E8%A1%8C%E5%88%97%E8%BD%AC%E6%8D%A2/</id>
    <published>2022-09-21T01:48:29.000Z</published>
    <updated>2022-09-21T02:56:33.462Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sql-行列转换"><a href="#sql-行列转换" class="headerlink" title="sql-行列转换"></a>sql-行列转换</h1><p>使用spark进行行列转换 主要有两种使用方式：</p><h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><p>测试数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">+----+----+----+</span><br><span class="line">|科目|姓名|分数|</span><br><span class="line">+----+----+----+</span><br><span class="line">|数学|张三|  88|</span><br><span class="line">|数学|李雷|  67|</span><br><span class="line">|数学|宫九|  77|</span><br><span class="line">|数学|王五|  65|</span><br><span class="line">|英语|张三|  77|</span><br><span class="line">|英语|宫九|  90|</span><br><span class="line">|英语|李雷|  24|</span><br><span class="line">|英语|王五|  90|</span><br><span class="line">|语文|李雷|  33|</span><br><span class="line">|语文|宫九|  87|</span><br><span class="line">|语文|张三|  92|</span><br><span class="line">|语文|王五|  87|</span><br><span class="line">+----+----+----+</span><br></pre></td></tr></table></figure><h3 id="使用PIVOT函数即可实现行转列"><a href="#使用PIVOT函数即可实现行转列" class="headerlink" title="使用PIVOT函数即可实现行转列"></a>使用PIVOT函数即可实现行转列</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> scores </span><br><span class="line">                 <span class="keyword">pivot</span></span><br><span class="line">                 (</span><br><span class="line">                     <span class="keyword">sum</span>(<span class="string">`分数`</span>) <span class="keyword">for</span></span><br><span class="line">                     <span class="string">`姓名`</span> <span class="keyword">in</span> (<span class="string">'张三'</span>,<span class="string">'王五'</span>,<span class="string">'李雷'</span>,<span class="string">'宫九'</span>)</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure><p>上边语句的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----+----+----+----+----+</span><br><span class="line">|科目|张三|王五|李雷|宫九|</span><br><span class="line">+----+----+----+----+----+</span><br><span class="line">|数学|  88|  65|  67|  77|</span><br><span class="line">|英语|  77|  90|  24|  90|</span><br><span class="line">|语文|  92|  87|  33|  87|</span><br><span class="line">+----+----+----+----+----+</span><br></pre></td></tr></table></figure><h3 id="stack使用"><a href="#stack使用" class="headerlink" title="stack使用"></a>stack使用</h3><p>stack(n, expr1, …, exprk) - 会将expr1, …, exprk 分割为n行.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">df_pivot.createOrReplaceTempView(<span class="symbol">'v_pivo</span>t')</span><br><span class="line"></span><br><span class="line">sql_content = ''<span class="symbol">'select</span> `科目`,</span><br><span class="line">                 stack(<span class="number">4</span>, '张三', `张三`, '王五', `王五`, '李雷', `李雷`, '宫九', `宫九`) as (`姓名`, `分数` )</span><br><span class="line">                 from  v_pivot             </span><br><span class="line">              '''</span><br><span class="line"></span><br><span class="line">df_unpivot1 = spark.sql(sql_content)</span><br><span class="line"></span><br><span class="line">df_unpivot1.show()</span><br></pre></td></tr></table></figure><p>result :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">+----+----+----+</span><br><span class="line">|科目|姓名|分数|</span><br><span class="line">+----+----+----+</span><br><span class="line">|数学|张三|  88|</span><br><span class="line">|数学|王五|  65|</span><br><span class="line">|数学|李雷|  67|</span><br><span class="line">|数学|宫九|  77|</span><br><span class="line">|英语|张三|  77|</span><br><span class="line">|英语|王五|  90|</span><br><span class="line">|英语|李雷|  24|</span><br><span class="line">|英语|宫九|  90|</span><br><span class="line">|语文|张三|  92|</span><br><span class="line">|语文|王五|  87|</span><br><span class="line">|语文|李雷|  33|</span><br><span class="line">|语文|宫九|  87|</span><br><span class="line">+----+----+----+</span><br></pre></td></tr></table></figure><p>这个函数不咋常用，但是在一些场景还是可以直接被用来显示效果。</p><h3 id="lateral-view-方式"><a href="#lateral-view-方式" class="headerlink" title="lateral view 方式"></a>lateral view 方式</h3><p>测试数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A 20 篮球，排球，乒乓球</span><br><span class="line">B 30 跳舞，唱歌</span><br><span class="line">C 23 唱歌，爬山</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql("<span class="keyword">select</span> <span class="keyword">name</span>,age,t.hobby <span class="keyword">from</span> sr2 <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(hobby,<span class="string">'，'</span>)) t <span class="keyword">as</span> hobby<span class="string">").show()</span></span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">+----+---+-----+</span><br><span class="line">|name|age|hobby|</span><br><span class="line">+----+---+-----+</span><br><span class="line">|   A| 20|   篮球|</span><br><span class="line">|   A| 20|   排球|</span><br><span class="line">|   A| 20|  乒乓球|</span><br><span class="line">|   B| 30|   跳舞|</span><br><span class="line">|   B| 30|   唱歌|</span><br><span class="line">|   C| 23|   唱歌|</span><br><span class="line">|   C| 23|   爬山|</span><br><span class="line">+----+---+-----+</span><br></pre></td></tr></table></figure><h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><p>测试数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2018-01,项目1,100</span><br><span class="line">2018-01,项目2,200</span><br><span class="line">2018-01,项目3,300</span><br><span class="line">2018-01,项目3,400</span><br><span class="line">2018-02,项目1,1000</span><br><span class="line">2018-02,项目2,2000</span><br><span class="line">2018-03,项目x,999</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql("<span class="keyword">select</span> yue, collect_set(<span class="keyword">project</span>) projects,<span class="keyword">sum</span>(shouru) zsr  <span class="keyword">from</span> sr <span class="keyword">group</span> <span class="keyword">by</span> yue<span class="string">").show()</span></span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+-------+---------------+----+</span><br><span class="line">|    yue|       projects| zsr|</span><br><span class="line">+-------+---------------+----+</span><br><span class="line">|2018-03|          [项目x]| 999|</span><br><span class="line">|2018-02|     [项目1, 项目2]|3000|</span><br><span class="line">|2018-01|[项目1, 项目2, 项目3]| 600|</span><br></pre></td></tr></table></figure><p>如果需要转化成字符串，可以使用<strong>concat_ws(‘分隔符’,字段)</strong> 函数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;sql-行列转换&quot;&gt;&lt;a href=&quot;#sql-行列转换&quot; class=&quot;headerlink&quot; title=&quot;sql-行列转换&quot;&gt;&lt;/a&gt;sql-行列转换&lt;/h1&gt;&lt;p&gt;使用spark进行行列转换 主要有两种使用方式：&lt;/p&gt;
&lt;h2 id=&quot;行转列&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="sql" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/sql/"/>
    
  </entry>
  
  <entry>
    <title>spark-top算子</title>
    <link href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/04/spark-top%E7%AE%97%E5%AD%90/"/>
    <id>https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/09/04/spark-top%E7%AE%97%E5%AD%90/</id>
    <published>2022-09-04T15:18:18.000Z</published>
    <updated>2022-09-26T12:59:09.476Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark-top算子"><a href="#spark-top算子" class="headerlink" title="spark top算子"></a>spark top算子</h1><p>top()函数的源码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  takeOrdered(num)(ord.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>top()调用takeOrdered()的源码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123; <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">   <span class="type">Array</span>.empty</span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   <span class="keyword">val</span> mapRDDs = mapPartitions &#123; items =&gt;</span><br><span class="line">     <span class="keyword">val</span> queue = <span class="keyword">new</span> <span class="type">BoundedPriorityQueue</span>[<span class="type">T</span>](num)(ord.reverse)</span><br><span class="line">     queue ++= util.collection.<span class="type">Utils</span>.takeOrdered(items, num)(ord)</span><br><span class="line">     <span class="type">Iterator</span>.single(queue)</span><br><span class="line">   &#125;   </span><br><span class="line">   <span class="keyword">if</span> (mapRDDs.partitions.length == <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="type">Array</span>.empty</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     mapRDDs.reduce &#123; (queue1, queue2) =&gt;</span><br><span class="line">       queue1 ++= queue2</span><br><span class="line">       queue1</span><br><span class="line">     &#125;.toArray.sorted(ord)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>问题分析： top()算子底层调用了 takeOrdered()这个函数，这个函数也是RDD中的一个算子，来看看上边的源码：<br>首先takeOrdered()里调用了 mapPartitions()，也就是说使用top()的时候会对我们第二次输出的结果进行分区，默认为2个分区，所以看到第三步的结果应该是每个分区的top(5)（这里我想的对不对，还有待商榷）；  其次top()会对我之前sortBy()的结果按照key重新排序，所以导致了我们Top N的结果不准确；</p><p>解决方案：<br>方案一：指定top()的排序方法，这里我们直接根据value排序：sortBy(x =&gt; x._2,false).top(10)(Ordering.by(e =&gt; e._2))<br>方案二：不用top()，直接用sortBy(x =&gt;x._2,false).take(10)<br>方案三：既然top()底层调用的是takeOrdered()，我们也直接可以用takeOrdered(10)(Ordering.by(e =&gt; e._2))<br><strong>思考：方案一中，我们既然指定了top()的排序方式，还需要sortBy()嘛？？？当然可以不用啊！！！所以我们可以去掉sortBy()</strong></p><blockquote><p>科普一下：top(10)(Ordering.by(e =&gt; e._2)) 这种写法叫做函数的柯里化。<br>柯里化(Currying)：把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，并且返回接受余下的参数且返回结果的新函数的技术。这个技术由 Christopher Strachey 以逻辑学家 Haskell Curry 命名的，尽管它是 Moses Schnfinkel 和 Gottlob Frege 发明的。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;spark-top算子&quot;&gt;&lt;a href=&quot;#spark-top算子&quot; class=&quot;headerlink&quot; title=&quot;spark top算子&quot;&gt;&lt;/a&gt;spark top算子&lt;/h1&gt;&lt;p&gt;top()函数的源码：&lt;/p&gt;
&lt;figure class=&quot;hig</summary>
      
    
    
    
    <category term="大数据" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
    <category term="spark" scheme="https://github.com/louzhiqiang/louzhiqiang.github.io.git/tags/spark/"/>
    
  </entry>
  
</feed>
