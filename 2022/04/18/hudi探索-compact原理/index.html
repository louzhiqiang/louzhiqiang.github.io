<!DOCTYPE HTML>
<html lang="zh-Hans">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="hudi探索--compact原理, 痒痒 团团 和 咘咘">
    <meta name="description" content="hudi探索–compact原理compaction 操作是针对于MOR表类型独有的操作，老生常谈的小文件，是必须要进行合并成大文件的，这样读的效率才更高。这样的思路在hbase等系统里也都是常规操作。
compact的过程hudi针对co">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>hudi探索--compact原理 | 痒痒 团团 和 咘咘</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="痒痒 团团 和 咘咘" type="application/atom+xml">
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">痒痒 团团 和 咘咘</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>Contact</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">痒痒 团团 和 咘咘</div>
        <div class="logo-desc">
            
            从自律开始
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			About
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			Contact
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">hudi探索--compact原理</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/hudi/">
                                <span class="chip bg-color">hudi</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                大数据
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2022-04-18
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="hudi探索–compact原理"><a href="#hudi探索–compact原理" class="headerlink" title="hudi探索–compact原理"></a>hudi探索–compact原理</h2><p>compaction 操作是针对于MOR表类型独有的操作，老生常谈的小文件，是必须要进行合并成大文件的，这样读的效率才更高。这样的思路在hbase等系统里也都是常规操作。</p>
<h3 id="compact的过程"><a href="#compact的过程" class="headerlink" title="compact的过程"></a>compact的过程</h3><p>hudi针对compact主要分成两个过程，生成 <code>HoodieCompactionPlan</code>和执行 <code>HoodieCompactionPlan</code>两阶段。这个过程是通用的，也比较抽象，compact的策略就可以根据上下文有很多的灵活性，我们就可以根据不同的业务场景，来指定自己的compact策略，如果没有特殊要求，默认策略也是很好的。</p>
<h4 id="生成-HoodieCompactionPlan"><a href="#生成-HoodieCompactionPlan" class="headerlink" title="生成 HoodieCompactionPlan"></a>生成 HoodieCompactionPlan</h4><p>生成计划是在调度的时候直接生成的，代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Option&lt;String&gt; <span class="title">scheduleCompaction</span><span class="params">(Option&lt;Map&lt;String, String&gt;&gt; extraMetadata)</span> <span class="keyword">throws</span> HoodieIOException </span>&#123;</span><br><span class="line">    String instantTime = HoodieActiveTimeline.createNewInstantTime();</span><br><span class="line">    <span class="keyword">return</span> scheduleCompactionAtInstant(instantTime, extraMetadata) ? Option.of(instantTime) : Option.empty();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>hudi 在 0.10.0 的版本中，（没有看之前的版本实现哈），针对 compact、clean、cluster 是有一个调度服务的，三者共用一个。这三者都属于tableService，</p>
<p>三者的模式也都是先生产plan，然后根据调度执行plan。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Option&lt;String&gt; <span class="title">scheduleTableServiceInternal</span><span class="params">(String instantTime, Option&lt;Map&lt;String, String&gt;&gt; extraMetadata,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                      TableServiceType tableServiceType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (tableServiceType) &#123;</span><br><span class="line">      <span class="keyword">case</span> CLUSTER:</span><br><span class="line">        LOG.info(<span class="string">"Scheduling clustering at instant time :"</span> + instantTime);</span><br><span class="line">        Option&lt;HoodieClusteringPlan&gt; clusteringPlan = createTable(config, hadoopConf, config.isMetadataTableEnabled())</span><br><span class="line">            .scheduleClustering(context, instantTime, extraMetadata);</span><br><span class="line">        <span class="keyword">return</span> clusteringPlan.isPresent() ? Option.of(instantTime) : Option.empty();</span><br><span class="line">      <span class="keyword">case</span> COMPACT:</span><br><span class="line">        LOG.info(<span class="string">"Scheduling compaction at instant time :"</span> + instantTime);</span><br><span class="line">        Option&lt;HoodieCompactionPlan&gt; compactionPlan = createTable(config, hadoopConf, config.isMetadataTableEnabled())</span><br><span class="line">            .scheduleCompaction(context, instantTime, extraMetadata);</span><br><span class="line">        <span class="keyword">return</span> compactionPlan.isPresent() ? Option.of(instantTime) : Option.empty();</span><br><span class="line">      <span class="keyword">case</span> CLEAN:</span><br><span class="line">        LOG.info(<span class="string">"Scheduling cleaning at instant time :"</span> + instantTime);</span><br><span class="line">        Option&lt;HoodieCleanerPlan&gt; cleanerPlan = createTable(config, hadoopConf, config.isMetadataTableEnabled())</span><br><span class="line">            .scheduleCleaning(context, instantTime, extraMetadata);</span><br><span class="line">        <span class="keyword">return</span> cleanerPlan.isPresent() ? Option.of(instantTime) : Option.empty();</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Invalid TableService "</span> + tableServiceType);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的createtable()，会根据不同的客户端，比如flink或者spark，来创建不同的表，我这边调试目前主要是flink，所以这里跟到的实体就是  HoodieFlinkMergeOnReadTable ， 根据这个实体进行的计划创建。</p>
<blockquote>
<p>不同的实体数据，操作和合并算法是有区别的，是否会有冲突还需要细看一下源码。</p>
</blockquote>
<p>在上边说的实体表里，调度compact的方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Option&lt;HoodieCompactionPlan&gt; <span class="title">scheduleCompaction</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieEngineContext context,</span></span></span><br><span class="line"><span class="function"><span class="params">      String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">      Option&lt;Map&lt;String, String&gt;&gt; extraMetadata)</span> </span>&#123;</span><br><span class="line">    ScheduleCompactionActionExecutor scheduleCompactionExecutor = <span class="keyword">new</span> ScheduleCompactionActionExecutor(</span><br><span class="line">        context, config, <span class="keyword">this</span>, instantTime, extraMetadata,</span><br><span class="line">        <span class="keyword">new</span> HoodieFlinkMergeOnReadTableCompactor());</span><br><span class="line">    <span class="keyword">return</span> scheduleCompactionExecutor.execute();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ScheduleCompactionActionExecutor 是一个模式类，里边主要是传入了一个compactor，然后都在execute里去生成计划。</p>
<p>execute的主要逻辑如下：</p>
<ul>
<li><p>判断是否需要compact，这部分主要是看compact的触发策略是否达到要求。</p>
<ul>
<li><p>跟当前这个instant time相比，有已经commited 的 instant time</p>
</li>
<li><p>getLatestDeltaCommitInfo方法会返回上次compact之后的deltaCommit的数量，以及 最近的一次deltacommit的instant time。是 一个二元组。然后再needCompact中，会根据第一个值以及配置 hoodie.compact.inline.max.delta.commits（默认是5），来比对确认是否达到要求。根据第二个值跟配置 hoodie.compact.inline.max.delta.seconds （默认360s） 进行对比，超过这个时间阈值，就可以compact。第一个配置是距离上次compact之后delta commits的数量，后者是距离上次compact的秒数。</p>
<p>上边的策略实际是两种，但是触发的时候我们有四种：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> CompactionTriggerStrategy &#123;</span><br><span class="line">    <span class="comment">// trigger compaction when reach N delta commits</span></span><br><span class="line">    NUM_COMMITS,</span><br><span class="line">    <span class="comment">// trigger compaction when time elapsed &gt; N seconds since last compaction</span></span><br><span class="line">    TIME_ELAPSED,</span><br><span class="line">    <span class="comment">// trigger compaction when both NUM_COMMITS and TIME_ELAPSED are satisfied</span></span><br><span class="line">    NUM_AND_TIME,</span><br><span class="line">    <span class="comment">// trigger compaction when NUM_COMMITS or TIME_ELAPSED is satisfied</span></span><br><span class="line">    NUM_OR_TIME</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>确认需要compact之后，就需要生成计划。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();</span><br><span class="line">        Set&lt;HoodieFileGroupId&gt; fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()</span><br><span class="line">            .map(instantTimeOpPair -&gt; instantTimeOpPair.getValue().getFileGroupId())</span><br><span class="line">            .collect(Collectors.toSet());</span><br><span class="line">        <span class="comment">// exclude files in pending clustering from compaction.</span></span><br><span class="line">        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));</span><br><span class="line">        <span class="keyword">return</span> compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> HoodieCompactionException(<span class="string">"Could not schedule compaction "</span> + config.getBasePath(), e);</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<pre><code>最终生成计划实际还是通过 这个 compactor。这个就是上边生成ScheduleCompactionActionExecutor的时候传入的compactor。generateCompactionPlan之前的那几行代码主要就是为了找到需要合并的文件。按照fileGroupId进行组织。

 generateCompactionPlan的主要逻辑如下：

 1. 校验表类型必须是 MERGE_ON_READ

 2. 获取合并策略并过滤分区。策略的配置是  hoodie.compaction.strategy。默认是 LogFileSizeBasedCompactionStrategy.class  这个配置的解释如下 ： Compaction strategy decides which file groups are picked up for compaction during each compaction run. By default. Hudi picks the log file with most accumulated unmerged data。

 LogFileSizeBasedCompactionStrategy 的 注解如下 ：LogFileSizeBasedCompactionStrategy orders the compactions based on the total log files size,filters the file group which log files size is greater than the threshold and limits the compactions within a configured IO bound. 这个类的策略是按照log file的大小来过滤并且排序。他也会过滤分区，为了增加compact的并行度，有了分区之后就可以多个点并行compact，因为分区之间是没有相关性的。

 3.根据hudi的文件视图，生成一个个的HoodieCompactionOperation，一个fileslice一个HoodieCompactionOperation。这个LIST是不区分partition的了。

 4.在上边那一步，会构建一个avro格式的元数据，包括这次compact的所有相关信息，包括统计等值。

 5. 生成执行计划，代码如下：</code></pre></li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HoodieCompactionPlan compactionPlan = config.getCompactionStrategy().generateCompactionPlan(config, operations,</span><br><span class="line">      CompactionUtils.getAllPendingCompactionPlans(metaClient).stream().map(Pair::getValue).collect(toList()));</span><br></pre></td></tr></table></figure>

<p>这里调用的 CompactionStrategy 是上边提到的logFile  默认的配置策略。调用的generateCompactionPlan 实际是 抽象类 CompactionStrategy 里的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> HoodieCompactionPlan <span class="title">generateCompactionPlan</span><span class="params">(HoodieWriteConfig writeConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">      List&lt;HoodieCompactionOperation&gt; operations, List&lt;HoodieCompactionPlan&gt; pendingCompactionPlans)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Strategy implementation can overload this method to set specific compactor-id</span></span><br><span class="line">    <span class="keyword">return</span> HoodieCompactionPlan.newBuilder()</span><br><span class="line">        .setOperations(orderAndFilter(writeConfig, operations, pendingCompactionPlans))</span><br><span class="line">        .setVersion(CompactionUtils.LATEST_COMPACTION_METADATA_VERSION).build();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的一个设计在于，他把可能变化的逻辑抽象出来之后，用户可以操作的恰好是用户需要的，而其他都封装好了。</p>
<p>比如这里的 orderAndFilter 方法，基本都是各个子策略自己实现的。在 上边说的 LogFileSizeBasedCompactionStrategy 中，orderAndFilter规定，log fize带下必须大于 配置 hoodie.compaction.logfile.size.threshold （默认是0字节） 的大小，才可以进行compact。然后在 BoundedIOCompactionStrategy 中也定义了 整个合并计划里涉及的总IO 大小， 不能超过 配置 hoodie.compaction.target.io （默认 500G ） 的大小，否则就会抛弃，其他的就会等待下次调度。</p>
<p>还有一个校验是 同一个fileId的合并不能出现在多个正在执行的compactPlan里，否则会出现并发影响。如果一个fileId出现在之前的compact里，那就直接抛出异常，当前的计划就终止了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ValidationUtils.checkArgument(</span><br><span class="line">        compactionPlan.getOperations().stream().noneMatch(</span><br><span class="line">            op -&gt; fgIdsInPendingCompactionAndClustering.contains(<span class="keyword">new</span> HoodieFileGroupId(op.getPartitionPath(), op.getFileId()))),</span><br><span class="line">        <span class="string">"Bad Compaction Plan. FileId MUST NOT have multiple pending compactions. "</span></span><br><span class="line">            + <span class="string">"Please fix your strategy implementation. FileIdsWithPendingCompactions :"</span> + fgIdsInPendingCompactionAndClustering</span><br><span class="line">            + <span class="string">", Selected workload :"</span> + compactionPlan);</span><br></pre></td></tr></table></figure>

<p>这里看到 fgIdsInPendingCompactionAndClustering 其实就是之前的正在pending中的计划的相关数据。</p>
<blockquote>
<p>fgIdsInPendingCompactionAndClustering 是所有之前生成的compact，状态是未完成的，然后打平出来的一个hashMap &lt;instantTime, compactionPlan&gt;,最后在ScheduleCompactionActionExecutor中，打平成FileGroupId</p>
</blockquote>
<blockquote>
<p>用来获取pending状态的compact使用的fileSystemView 是 在HoodieTable里 通过方法 createViewManager 创建的，根据配置 view 在服务中的存储方式来构建，如下边代码中，有五种，默认是MEMORY。</p>
<p>这个配置 hoodie.filesystem.view.type ，注释如下：File system view provides APIs for viewing the files on the underlying lake storage,as file groups and file slices.This config controls how such a view is held. 他们provide different trade offs for memory usage and API request performance.</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">enum</span> FileSystemViewStorageType &#123;</span><br><span class="line">  <span class="comment">// In-memory storage of file-system view</span></span><br><span class="line">  MEMORY,</span><br><span class="line">  <span class="comment">// Constrained Memory storage for file-system view with overflow data spilled to disk</span></span><br><span class="line">  SPILLABLE_DISK,</span><br><span class="line">  <span class="comment">// EMBEDDED Key Value Storage for file-system view</span></span><br><span class="line">  EMBEDDED_KV_STORE,</span><br><span class="line">  <span class="comment">// Delegate file-system view to remote server</span></span><br><span class="line">  REMOTE_ONLY,</span><br><span class="line">  <span class="comment">// A composite storage where file-system view calls are first delegated to Remote server ( REMOTE_ONLY )</span></span><br><span class="line">  <span class="comment">// In case of failures, switches subsequent calls to secondary local storage type</span></span><br><span class="line">  REMOTE_FIRST</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>关于memory格式的表，内部会根据配置hoodie.metadata.enable 来判断需要生成 HoodieMetadataFileSystemView 还是HoodieTableFileSystemView。这个配置默认是false。注释如下：Enable the internal metadata table which serves table metadata like level file listings。</p>
<p>我这边没管这个配置，应该是默认值。所以这里走的是 HoodieTableFileSystemView。</p>
<p>生成的时候会调用父类的 init 方法，里边会生成  fgIdToPendingCompaction，</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">resetPendingCompactionOperations(CompactionUtils.getAllPendingCompactionOperations(metaClient).values().stream()</span><br><span class="line">        .map(e -&gt; Pair.of(e.getKey(), CompactionOperation.convertFromAvroRecordInstance(e.getValue()))));</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Map&lt;HoodieFileGroupId, Pair&lt;String, HoodieCompactionOperation&gt;&gt; getAllPendingCompactionOperations(</span><br><span class="line">      HoodieTableMetaClient metaClient) &#123;</span><br><span class="line">    List&lt;Pair&lt;HoodieInstant, HoodieCompactionPlan&gt;&gt; pendingCompactionPlanWithInstants =</span><br><span class="line">        getAllPendingCompactionPlans(metaClient);</span><br><span class="line"></span><br><span class="line">    Map&lt;HoodieFileGroupId, Pair&lt;String, HoodieCompactionOperation&gt;&gt; fgIdToPendingCompactionWithInstantMap =</span><br><span class="line">        <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    pendingCompactionPlanWithInstants.stream().flatMap(instantPlanPair -&gt;</span><br><span class="line">        getPendingCompactionOperations(instantPlanPair.getKey(), instantPlanPair.getValue())).forEach(pair -&gt; &#123;</span><br><span class="line">          <span class="comment">// Defensive check to ensure a single-fileId does not have more than one pending compaction with different</span></span><br><span class="line">          <span class="comment">// file slices. If we find a full duplicate we assume it is caused by eventual nature of the move operation</span></span><br><span class="line">          <span class="comment">// on some DFSs.</span></span><br><span class="line">          <span class="keyword">if</span> (fgIdToPendingCompactionWithInstantMap.containsKey(pair.getKey())) &#123;</span><br><span class="line">            HoodieCompactionOperation operation = pair.getValue().getValue();</span><br><span class="line">            HoodieCompactionOperation anotherOperation = fgIdToPendingCompactionWithInstantMap.get(pair.getKey()).getValue();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!operation.equals(anotherOperation)) &#123;</span><br><span class="line">              String msg = <span class="string">"Hudi File Id ("</span> + pair.getKey() + <span class="string">") has more than 1 pending compactions. Instants: "</span></span><br><span class="line">                  + pair.getValue() + <span class="string">", "</span> + fgIdToPendingCompactionWithInstantMap.get(pair.getKey());</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(msg);</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          fgIdToPendingCompactionWithInstantMap.put(pair.getKey(), pair.getValue());</span><br><span class="line">        &#125;);</span><br><span class="line">    <span class="keyword">return</span> fgIdToPendingCompactionWithInstantMap;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在这里 getAllPendingCompactionPlans 是获取pending状态的compact操作。pending就是未完成。</p>
<p>然后下边有一次打平操作。逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Stream&lt;Pair&lt;HoodieFileGroupId, Pair&lt;String, HoodieCompactionOperation&gt;&gt;&gt; getPendingCompactionOperations(</span><br><span class="line">      HoodieInstant instant, HoodieCompactionPlan compactionPlan) &#123;</span><br><span class="line">    List&lt;HoodieCompactionOperation&gt; ops = compactionPlan.getOperations();</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> != ops) &#123;</span><br><span class="line">      <span class="keyword">return</span> ops.stream().map(op -&gt; Pair.of(<span class="keyword">new</span> HoodieFileGroupId(op.getPartitionPath(), op.getFileId()),</span><br><span class="line">          Pair.of(instant.getTimestamp(), op)));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> Stream.empty();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在这里，他把fileid提取出来了。用来给后边的进行过滤。、</p>
<p>回到 execute() 方法，在生成 HoodieCompactionPlan 之后，会生成一个 hudiInstant，然后把这一些元数据写入hudi的 timeline。状态是 REQUESTED。会将其序列化后保存在 <code>.hoodie/.aux</code>元数据目录下。</p>
<p>记录一个类的结构图：</p>
<p><img src="/images/HoodieFlinkMergeOnReadTableCompactor.png" alt=""></p>
<p>这个是  HoodieFlinkMergeOnReadTableCompactor  的结构图，他继承了HoodieCompactor 。这个是实际执行的时候会用到的。</p>
<p>另一个类图  LogFileSizeBasedCompactionStrategy ：</p>
<p><img src="/images/LogFileSizeBasedCompactionStrategy.png" alt=""></p>
<p>这里看出，实际CompactionStrategy 的子类有多个，这个主要就是生成策略，确认好合并的路子之后，后边的执行是上边的compactor需要做的。如果需要扩展合并策略，就要实现 CompactionStrategy 这个类。</p>
<hr>
<h4 id="执行compactplan"><a href="#执行compactplan" class="headerlink" title="执行compactplan"></a>执行compactplan</h4><p>上边说了plan的生成，下边我们看一下真正执行的时候的逻辑是啥样的。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> O <span class="title">compact</span><span class="params">(String compactionInstantTime)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> compact(compactionInstantTime, config.shouldAutoCommit());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个代码是在 类  AbstractHoodieWriteClient 中，在这个方法中，用到了一个配置 ： hoodie.auto.commit  默认是 true  Controls whether a write operation should auto commit.his can be turned off to perform inspection of the uncommitted write before deciding to commit.</p>
<p>如果在提交之前有需要触发的动作，这个开关就要off掉。</p>
<p>这里调用的compact最终是在  HoodieFlinkWriteClient 里。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;WriteStatus&gt; writeStatuses =</span><br><span class="line">          getHoodieTable().compact(context, compactionInstantTime).getWriteStatuses();</span><br><span class="line">      commitCompaction(compactionInstantTime, writeStatuses, Option.empty());</span><br><span class="line">      <span class="keyword">return</span> writeStatuses;</span><br></pre></td></tr></table></figure>

<p>这里的 getHoodieTable() 是HoodieFlinkMergeOnReadTable  ，他的compact 逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> HoodieWriteMetadata&lt;List&lt;WriteStatus&gt;&gt; compact(</span><br><span class="line">      HoodieEngineContext context, String compactionInstantTime) &#123;</span><br><span class="line">    RunCompactionActionExecutor compactionExecutor = <span class="keyword">new</span> RunCompactionActionExecutor(</span><br><span class="line">        context, config, <span class="keyword">this</span>, compactionInstantTime, <span class="keyword">new</span> HoodieFlinkMergeOnReadTableCompactor(),</span><br><span class="line">        <span class="keyword">new</span> HoodieFlinkCopyOnWriteTable(config, context, getMetaClient()));</span><br><span class="line">    <span class="keyword">return</span> convertMetadata(compactionExecutor.execute());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的这个RunCompactionActionExecutor跟上边生成策略时的思路是一样的，继承了 BaseActionExecutor，他的主要执行也是在方法 execute 里。BaseActionExecutor主要是写了一些metastore相关的操作。</p>
<p>execute方法的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> HoodieWriteMetadata&lt;HoodieData&lt;WriteStatus&gt;&gt; execute() &#123;</span><br><span class="line">    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();</span><br><span class="line">    <span class="comment">// compactor 是 HoodieFlinkMergeOnReadTableCompactor</span></span><br><span class="line">    <span class="comment">// preCompact 如果有一些instant在别的正在运行的compactplan中，那么就要把正在进行中的compact进行回滚</span></span><br><span class="line">    compactor.preCompact(table, pendingCompactionTimeline, instantTime);</span><br><span class="line"></span><br><span class="line">    HoodieWriteMetadata&lt;HoodieData&lt;WriteStatus&gt;&gt; compactionMetadata = <span class="keyword">new</span> HoodieWriteMetadata&lt;&gt;();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// generate compaction plan</span></span><br><span class="line">      <span class="comment">// should support configurable commit metadata</span></span><br><span class="line">      <span class="comment">// 这段代码就是从之前.hoodie/.aux下获取对应的之前存储的compact的元数据</span></span><br><span class="line">      HoodieCompactionPlan compactionPlan =</span><br><span class="line">          CompactionUtils.getCompactionPlan(table.getMetaClient(), instantTime);</span><br><span class="line"></span><br><span class="line">      HoodieData&lt;WriteStatus&gt; statuses = compactor.compact(</span><br><span class="line">          context, compactionPlan, table, config, instantTime, compactionHandler);</span><br><span class="line"></span><br><span class="line">      compactor.maybePersist(statuses, config);</span><br><span class="line">      context.setJobStatus(<span class="keyword">this</span>.getClass().getSimpleName(), <span class="string">"Preparing compaction metadata"</span>);</span><br><span class="line">      List&lt;HoodieWriteStat&gt; updateStatusMap = statuses.map(WriteStatus::getStat).collectAsList();</span><br><span class="line">      HoodieCommitMetadata metadata = <span class="keyword">new</span> HoodieCommitMetadata(<span class="keyword">true</span>);</span><br><span class="line">      <span class="keyword">for</span> (HoodieWriteStat stat : updateStatusMap) &#123;</span><br><span class="line">        metadata.addWriteStat(stat.getPartitionPath(), stat);</span><br><span class="line">      &#125;</span><br><span class="line">      metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, config.getSchema());</span><br><span class="line"></span><br><span class="line">      compactionMetadata.setWriteStatuses(statuses);</span><br><span class="line">      compactionMetadata.setCommitted(<span class="keyword">false</span>);</span><br><span class="line">      compactionMetadata.setCommitMetadata(Option.of(metadata));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieCompactionException(<span class="string">"Could not compact "</span> + config.getBasePath(), e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> compactionMetadata;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里看一下那个compact的逻辑，代码是在HoodieCompactor中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Execute compaction operations and report back status.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> HoodieData&lt;WriteStatus&gt; <span class="title">compact</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieEngineContext context, HoodieCompactionPlan compactionPlan,</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieTable table, HoodieWriteConfig config, String compactionInstantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">      HoodieCompactionHandler compactionHandler)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (compactionPlan == <span class="keyword">null</span> || (compactionPlan.getOperations() == <span class="keyword">null</span>)</span><br><span class="line">        || (compactionPlan.getOperations().isEmpty())) &#123;</span><br><span class="line">      <span class="keyword">return</span> context.emptyHoodieData();</span><br><span class="line">    &#125;</span><br><span class="line">    HoodieActiveTimeline timeline = table.getActiveTimeline();</span><br><span class="line">    HoodieInstant instant = HoodieTimeline.getCompactionRequestedInstant(compactionInstantTime);</span><br><span class="line">    <span class="comment">// Mark instant as compaction inflight</span></span><br><span class="line">    timeline.transitionCompactionRequestedToInflight(instant);</span><br><span class="line">    table.getMetaClient().reloadActiveTimeline();</span><br><span class="line"></span><br><span class="line">    HoodieTableMetaClient metaClient = table.getMetaClient();</span><br><span class="line">    TableSchemaResolver schemaUtil = <span class="keyword">new</span> TableSchemaResolver(metaClient);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Here we firstly use the table schema as the reader schema to read</span></span><br><span class="line">    <span class="comment">// log file.That is because in the case of MergeInto, the config.getSchema may not</span></span><br><span class="line">    <span class="comment">// the same with the table schema.</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      Schema readerSchema = schemaUtil.getTableAvroSchema(<span class="keyword">false</span>);</span><br><span class="line">      config.setSchema(readerSchema.toString());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="comment">// If there is no commit in the table, just ignore the exception.</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compacting is very similar to applying updates to existing file</span></span><br><span class="line">    List&lt;CompactionOperation&gt; operations = compactionPlan.getOperations().stream()</span><br><span class="line">        .map(CompactionOperation::convertFromAvroRecordInstance).collect(toList());</span><br><span class="line">    LOG.info(<span class="string">"Compactor compacting "</span> + operations + <span class="string">" files"</span>);</span><br><span class="line"></span><br><span class="line">    context.setJobStatus(<span class="keyword">this</span>.getClass().getSimpleName(), <span class="string">"Compacting file slices"</span>);</span><br><span class="line">    TaskContextSupplier taskContextSupplier = table.getTaskContextSupplier();</span><br><span class="line">    <span class="keyword">return</span> context.parallelize(operations).map(operation -&gt; compact(</span><br><span class="line">        compactionHandler, metaClient, config, operation, compactionInstantTime, taskContextSupplier))</span><br><span class="line">        .flatMap(List::iterator);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在最后一行代码之前，主要还是转化了schema的问题，这块还是要从schema整体的抽象去考虑。</p>
<blockquote>
<p>hudi 中 schama 如何管理的?</p>
</blockquote>
<p>最终任务是通过引擎，进行并行执行compact，所以这里合并是在 compact方法进行具体的执行。按照一个FileSlice进行合并的。</p>
<p>这个compact的代码比较多，我罗列一下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Execute a single compaction operation and report back status.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;WriteStatus&gt; <span class="title">compact</span><span class="params">(HoodieCompactionHandler compactionHandler,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   HoodieTableMetaClient metaClient,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   HoodieWriteConfig config,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   CompactionOperation operation,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   String instantTime,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   TaskContextSupplier taskContextSupplier)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    FileSystem fs = metaClient.getFs();</span><br><span class="line"></span><br><span class="line">    Schema readerSchema = HoodieAvroUtils.addMetadataFields(</span><br><span class="line">        <span class="keyword">new</span> Schema.Parser().parse(config.getSchema()), config.allowOperationMetadataField());</span><br><span class="line">    LOG.info(<span class="string">"Compacting base "</span> + operation.getDataFileName() + <span class="string">" with delta files "</span> + operation.getDeltaFileNames()</span><br><span class="line">        + <span class="string">" for commit "</span> + instantTime);</span><br><span class="line">    <span class="comment">// TODO - FIX THIS</span></span><br><span class="line">    <span class="comment">// Reads the entire avro file. Always only specific blocks should be read from the avro file</span></span><br><span class="line">    <span class="comment">// (failure recover).</span></span><br><span class="line">    <span class="comment">// Load all the delta commits since the last compaction commit and get all the blocks to be</span></span><br><span class="line">    <span class="comment">// loaded and load it using CompositeAvroLogReader</span></span><br><span class="line">    <span class="comment">// Since a DeltaCommit is not defined yet, reading all the records. revisit this soon.</span></span><br><span class="line">    String maxInstantTime = metaClient</span><br><span class="line">        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,</span><br><span class="line">            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))</span><br><span class="line">        .filterCompletedInstants().lastInstant().get().getTimestamp();</span><br><span class="line">    <span class="comment">// 如果配置了 hoodie.memory.compaction.max.size  默认是无</span></span><br><span class="line">    <span class="comment">//注释："Maximum amount of memory used for compaction operations, before spilling to local storage.</span></span><br><span class="line">    <span class="comment">// 如果上边那个没有配置，就会读取 hoodie.memory.compaction.fraction 这个配置，默认是0.6</span></span><br><span class="line">    <span class="comment">// 注释：HoodieCompactedLogScanner reads logblocks, converts records to HoodieRecords and then</span></span><br><span class="line">    <span class="comment">// merges these log blocks and records. At any point, the number of entries in a log block can be  </span></span><br><span class="line">    <span class="comment">// less than or equal to the number of entries in the corresponding parquet file.</span></span><br><span class="line">    <span class="comment">// This can lead to  OOM in the Scanner. Hence, a spillable map helps alleviate the memory pressure. Use this config to</span></span><br><span class="line">    <span class="comment">// set the max allowable inMemory footprint（内存占用） of the spillable map</span></span><br><span class="line">    <span class="keyword">long</span> maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(taskContextSupplier, config);</span><br><span class="line">    LOG.info(<span class="string">"MaxMemoryPerCompaction =&gt; "</span> + maxMemoryPerCompaction);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; logFiles = operation.getDeltaFileNames().stream().map(</span><br><span class="line">        p -&gt; <span class="keyword">new</span> Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())</span><br><span class="line">        .collect(toList());</span><br><span class="line">    <span class="comment">// 获取所有log相关的文件</span></span><br><span class="line">    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()</span><br><span class="line">        .withFileSystem(fs)</span><br><span class="line">        .withBasePath(metaClient.getBasePath())</span><br><span class="line">        .withLogFilePaths(logFiles)</span><br><span class="line">        .withReaderSchema(readerSchema)</span><br><span class="line">        .withLatestInstantTime(maxInstantTime)</span><br><span class="line">        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)</span><br><span class="line">        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())</span><br><span class="line">        .withReverseReader(config.getCompactionReverseLogReadEnabled())</span><br><span class="line">        .withBufferSize(config.getMaxDFSStreamBufferSize())</span><br><span class="line">        .withSpillableMapBasePath(config.getSpillableMapBasePath())</span><br><span class="line">        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())</span><br><span class="line">        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())</span><br><span class="line">        .withOperationField(config.allowOperationMetadataField())</span><br><span class="line">        .withPartition(operation.getPartitionPath())</span><br><span class="line">        .build();</span><br><span class="line">    <span class="keyword">if</span> (!scanner.iterator().hasNext()) &#123;</span><br><span class="line">      scanner.close();</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 获取parquet文件</span></span><br><span class="line">    Option&lt;HoodieBaseFile&gt; oldDataFileOpt =</span><br><span class="line">        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());</span><br><span class="line">    <span class="comment">// 以下逻辑按照数据更新流程来处理</span></span><br><span class="line">    <span class="comment">// 这里的 compactionHandler 是 HoodieFlinkCopyOnWriteTable</span></span><br><span class="line">    <span class="comment">// Compacting is very similar to applying updates to existing file</span></span><br><span class="line">    Iterator&lt;List&lt;WriteStatus&gt;&gt; result;</span><br><span class="line">    <span class="comment">// If the dataFile is present, perform updates else perform inserts into a new base file.</span></span><br><span class="line">    <span class="keyword">if</span> (oldDataFileOpt.isPresent()) &#123;</span><br><span class="line">      result = compactionHandler.handleUpdate(instantTime, operation.getPartitionPath(),</span><br><span class="line">          operation.getFileId(), scanner.getRecords(),</span><br><span class="line">          oldDataFileOpt.get());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      result = compactionHandler.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),</span><br><span class="line">          scanner.getRecords());</span><br><span class="line">    &#125;</span><br><span class="line">    scanner.close();</span><br><span class="line">    Iterable&lt;List&lt;WriteStatus&gt;&gt; resultIterable = () -&gt; result;</span><br><span class="line">    <span class="comment">// 关注一下这些统计，对于后续合并策略的调整有重要的作用</span></span><br><span class="line">    <span class="keyword">return</span> StreamSupport.stream(resultIterable.spliterator(), <span class="keyword">false</span>).flatMap(Collection::stream).peek(s -&gt; &#123;</span><br><span class="line">      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());</span><br><span class="line">      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());</span><br><span class="line">      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());</span><br><span class="line">      s.getStat().setPartitionPath(operation.getPartitionPath());</span><br><span class="line">      s.getStat()</span><br><span class="line">          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());</span><br><span class="line">      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());</span><br><span class="line">      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());</span><br><span class="line">      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());</span><br><span class="line">      RuntimeStats runtimeStats = <span class="keyword">new</span> RuntimeStats();</span><br><span class="line">      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());</span><br><span class="line">      s.getStat().setRuntimeStats(runtimeStats);</span><br><span class="line">    &#125;).collect(toList());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里顺带着回顾一下写的逻辑，handleInsert就不说了，直接看一下handleUpdate。</p>
<p>handleUpdate的方式的主要思路如下：</p>
<p>1、获取schema</p>
<p>2、HoodieFileReader的生成</p>
<p>3、创建 readerIterator ，主要是读取parquet相关的数据。</p>
<p>4、BoundedInMemoryExecutor 创建：一个固定长度的容器，会有生产者和消费者，生产者就是basefile的数据迭代器，而消费者就是mergeHandle，也就是HoodieMergeHandler，然后加了一个转换schema的函数。</p>
<p>5、execute  BoundedInMemoryExecutor执行</p>
<p>在HoodieMergeHandler初始化的时候，就把所有的log的数据进行了遍历，形成了&lt;recordKey, HoodieRecord&gt;的结构，这个数据是存储在一个 ExternalSpillableMap的结构里，也就是一个可以扩展到硬盘存储的hashMap。</p>
<p>在consumer消费的时候，也就是调用了HoodieMergeHandler的write方法。这个write方法传入的参数是parquet base file里的一条记录，旧记录。如果在增量数据中存在对一个的key，就要进行合并把新的记录写到新的文件里，反之就不合并，把就旧记录写入新的文件里。</p>
<blockquote>
<p>这里涉及比较琐碎的点是schema的合并。这块的逻辑应该会频繁考虑schema的变更带来的问题。也是一个适合考虑线上数据版本升级的问题。</p>
</blockquote>
<p>这里具体写入调用的是  HoodieParquetWriter，调用的parquet-hadoop相关代码包进行的数据写入。</p>
<p>以上基本就是compact的全过程。</p>
<blockquote>
<p>这里读取parquet的方式并不是向量化读取。</p>
</blockquote>
<blockquote>
<p>1、在计划生成之前会形成一个 为完成的compact的 以fileid为粒度的map</p>
<p>2、plan生成的时候会校验，当前的fileId不能在之前的未完成compact中存在。</p>
<p>3、在计划执行的时候，会校验当前的instant time，包装成 一个Inflight状态的HoodieInstant，判断这个HoodieInstant是不是在正在pending中的，如果在，那就把当前这个instantTime标识的整个调度进行回滚成Requested状态。</p>
</blockquote>
<p>参考：</p>
<p><a href="https://cloud.tencent.com/developer/article/1812134" target="_blank" rel="noopener">Hudi 压缩(Compaction)实现分析 - 云+社区 - 腾讯云</a></p>
<p><a href="https://www.cnblogs.com/leesf456/p/13658785.html" target="_blank" rel="noopener">Apache Hudi异步Compaction方式汇总 - leesf - 博客园</a></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">zhiqiang.lou</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/04/18/hudi%E6%8E%A2%E7%B4%A2-compact%E5%8E%9F%E7%90%86/">https://github.com/louzhiqiang/louzhiqiang.github.io.git/2022/04/18/hudi%E6%8E%A2%E7%B4%A2-compact%E5%8E%9F%E7%90%86/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint policy. If reproduced, please indicate source
                    <a href="/about" target="_blank">zhiqiang.lou</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/hudi/">
                                    <span class="chip bg-color">hudi</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2022/04/18/hudi%E6%8E%A2%E7%B4%A2-clean%E5%8E%9F%E7%90%86/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/2.jpg" class="responsive-img" alt="hudi探索--clean原理">
                        
                        <span class="card-title">hudi探索--clean原理</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-04-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/hudi/">
                        <span class="chip bg-color">hudi</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/04/15/flink%E5%8E%9F%E7%90%86-%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0EXACTLY-ONCE/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="flink原理--如何实现EXACTLY_ONCE">
                        
                        <span class="card-title">flink原理--如何实现EXACTLY_ONCE</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-04-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="post-category">
                                    大数据
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/flink/">
                        <span class="chip bg-color">flink</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2022</span>
            
            <a href="/about" target="_blank">zhiqiang.lou</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;Total visits:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;Total visitors:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/blinkfox" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1181062873@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1181062873" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1181062873" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
